{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Example of VAE on MNIST dataset using MLP\n",
    "The VAE has a modular design. The encoder, decoder and VAE\n",
    "are 3 models that share weights. After training the VAE model,\n",
    "the encoder can be used to  generate latent vectors.\n",
    "The decoder can be used to generate MNIST digits by sampling the\n",
    "latent vector from a Gaussian distribution with mean=0 and std=1.\n",
    "# Reference\n",
    "[1] Kingma, Diederik P., and Max Welling.\n",
    "\"Auto-encoding variational bayes.\"\n",
    "https://arxiv.org/abs/1312.6114\n",
    "'''\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Lambda, Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "class Dummy(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(Dummy, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.kernel = self.add_weight(name='kernel', \n",
    "                                      shape=(input_shape[1], self.output_dim),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        super(Dummy, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        return K.dot(x, self.kernel)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)\n",
    "\n",
    "# reparameterization trick\n",
    "# instead of sampling from Q(z|X), sample eps = N(0,I)\n",
    "# z = z_mean + sqrt(var)*eps\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "    # Arguments:\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    # Returns:\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "def plot_results(models,\n",
    "                 data,\n",
    "                 batch_size=128,\n",
    "                 model_name=\"vae_mnist\"):\n",
    "    \"\"\"Plots labels and MNIST digits as function of 2-dim latent vector\n",
    "    # Arguments:\n",
    "        models (tuple): encoder and decoder models\n",
    "        data (tuple): test data and label\n",
    "        batch_size (int): prediction batch size\n",
    "        model_name (string): which model is using this function\n",
    "    \"\"\"\n",
    "\n",
    "    encoder, decoder = models\n",
    "    x_test, y_test = data\n",
    "    os.makedirs(model_name, exist_ok=True)\n",
    "\n",
    "    filename = os.path.join(model_name, \"vae_mean.png\")\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    [z_mean_1, z_log_var_1, z_mean_2, z_log_var_2, z_1, z_2, predict_c] = encoder.predict(x_test,batch_size=batch_size)\n",
    "    \n",
    "    color = np.ones(1000)\n",
    "    for i in range(0, 1000):\n",
    "        if predict_c[i] < 0.5:\n",
    "            z_mean_1[i,:] = z_mean_2[i,:]\n",
    "            color[i] = 0\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(z_mean_1[:, 0], z_mean_1[:, 1], c=color)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dim = 8\n",
    "mean1 = np.zeros(8)\n",
    "mean2 = np.zeros(8)\n",
    "mean2[0] = 1\n",
    "mean2[1] = 1\n",
    "\n",
    "cov1 = np.diag(np.ones(8))*0.001\n",
    "cov2 = cov1\n",
    "cov1[0,0] = 0.2\n",
    "cov1[1,1] = 0.1\n",
    "cov2[0,0] = 0.1\n",
    "cov2[1,1] = 0.2\n",
    "\n",
    "x1 = np.random.multivariate_normal(mean1, cov1, 500)\n",
    "x2 = np.random.multivariate_normal(mean2, cov2, 500)\n",
    "x_train = np.concatenate((x1,x2), axis = 0)\n",
    "\n",
    "x1 = np.random.multivariate_normal(mean1, cov1, 500)\n",
    "x2 = np.random.multivariate_normal(mean2, cov2, 500)\n",
    "x_test = np.concatenate((x1,x2), axis = 0)\n",
    "y_test = np.zeros(1000)\n",
    "y_test[500:] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 8)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "z_mean_1 (Dense)                (None, 2)            18          encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var_1 (Dense)             (None, 2)            18          encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z_mean_2 (Dense)                (None, 2)            18          encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var_2 (Dense)             (None, 2)            18          encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dummy (InputLayer)              (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "z_1 (Lambda)                    (None, 2)            0           z_mean_1[0][0]                   \n",
      "                                                                 z_log_var_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "z_2 (Lambda)                    (None, 2)            0           z_mean_2[0][0]                   \n",
      "                                                                 z_log_var_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_164 (Dense)               (None, 1)            9           encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mu1 (Dense)                     (None, 2)            6           dummy[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "mu2 (Dense)                     (None, 2)            6           dummy[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 93\n",
      "Trainable params: 93\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "z_sampling_1 (InputLayer)       (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "z_sampling_2 (InputLayer)       (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_165 (Dense)               (None, 8)            24          z_sampling_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_166 (Dense)               (None, 8)            24          z_sampling_2[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 48\n",
      "Trainable params: 48\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# network parameters\n",
    "input_shape = (original_dim, )\n",
    "intermediate_dim = 4\n",
    "batch_size = 64\n",
    "latent_dim = 2\n",
    "cat_dim = 1\n",
    "epochs = 500\n",
    "\n",
    "# VAE model = encoder + decoder\n",
    "# build encoder model\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "dummy = Input(shape=(latent_dim,), name='dummy')\n",
    "\n",
    "mu1 = Dense(latent_dim, name='mu1')(dummy)\n",
    "mu2 = Dense(latent_dim, name='mu2')(dummy)\n",
    "\n",
    "# x = Dense(intermediate_dim)(inputs)\n",
    "\n",
    "z_mean_1 = Dense(latent_dim, name='z_mean_1')(inputs)\n",
    "z_log_var_1 = Dense(latent_dim, name='z_log_var_1')(inputs)\n",
    "z_mean_2 = Dense(latent_dim, name='z_mean_2')(inputs)\n",
    "z_log_var_2 = Dense(latent_dim, name='z_log_var_2')(inputs)\n",
    "\n",
    "c = Dense(cat_dim, activation='sigmoid')(inputs)\n",
    "\n",
    "# use reparameterization trick to push the sampling out as input\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z_1 = Lambda(sampling, output_shape=(latent_dim,), name='z_1')([z_mean_1, z_log_var_1])\n",
    "z_2 = Lambda(sampling, output_shape=(latent_dim,), name='z_2')([z_mean_2, z_log_var_2])\n",
    "\n",
    "# instantiate encoder model\n",
    "encoder = Model([inputs,dummy], [z_mean_1, z_log_var_1, z_mean_2, z_log_var_2, z_1, z_2, c, mu1, mu2], name='encoder')\n",
    "encoder.summary()\n",
    "#plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)\n",
    "\n",
    "# build decoder model\n",
    "latent_inputs_1 = Input(shape=(latent_dim,), name='z_sampling_1')\n",
    "#x_1 = Dense(intermediate_dim)(latent_inputs_1)\n",
    "outputs_1 = Dense(original_dim)(latent_inputs_1)\n",
    "\n",
    "latent_inputs_2 = Input(shape=(latent_dim,), name='z_sampling_2')\n",
    "#x_2 = Dense(intermediate_dim)(latent_inputs_2)\n",
    "outputs_2 = Dense(original_dim)(latent_inputs_2)\n",
    "\n",
    "# instantiate decoder model\n",
    "decoder = Model([latent_inputs_1, latent_inputs_2], [outputs_1, outputs_2], name='decoder')\n",
    "decoder.summary()\n",
    "#plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)\n",
    "\n",
    "# instantiate VAE model\n",
    "[outputs_1, outputs_2] = decoder(encoder([inputs,dummy])[4:6])\n",
    "vae = Model([inputs,dummy], [outputs_1, outputs_2], name='vae_mlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 8)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dummy (InputLayer)              (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Model)                 [(None, 2), (None, 2 93          encoder_input[0][0]              \n",
      "                                                                 dummy[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Model)                 [(None, 8), (None, 8 48          encoder[1][4]                    \n",
      "                                                                 encoder[1][5]                    \n",
      "==================================================================================================\n",
      "Total params: 141\n",
      "Trainable params: 141\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "Epoch 1/500\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 4.7976 - val_loss: 4.6519\n",
      "Epoch 2/500\n",
      "1000/1000 [==============================] - 0s 135us/step - loss: 4.3779 - val_loss: 4.1079\n",
      "Epoch 3/500\n",
      "1000/1000 [==============================] - 0s 123us/step - loss: 3.8316 - val_loss: 3.6982\n",
      "Epoch 4/500\n",
      "1000/1000 [==============================] - 0s 126us/step - loss: 3.4363 - val_loss: 3.3565\n",
      "Epoch 5/500\n",
      "1000/1000 [==============================] - 0s 121us/step - loss: 3.1501 - val_loss: 2.9097\n",
      "Epoch 6/500\n",
      "1000/1000 [==============================] - 0s 124us/step - loss: 2.8527 - val_loss: 2.8058\n",
      "Epoch 7/500\n",
      "1000/1000 [==============================] - 0s 124us/step - loss: 2.5734 - val_loss: 2.4188\n",
      "Epoch 8/500\n",
      "1000/1000 [==============================] - 0s 124us/step - loss: 2.3190 - val_loss: 2.2993\n",
      "Epoch 9/500\n",
      "1000/1000 [==============================] - 0s 127us/step - loss: 2.1646 - val_loss: 2.0517\n",
      "Epoch 10/500\n",
      "1000/1000 [==============================] - 0s 126us/step - loss: 1.9125 - val_loss: 1.9357\n",
      "Epoch 11/500\n",
      "1000/1000 [==============================] - 0s 130us/step - loss: 1.8442 - val_loss: 1.8624\n",
      "Epoch 12/500\n",
      "1000/1000 [==============================] - 0s 125us/step - loss: 1.6990 - val_loss: 1.6924\n",
      "Epoch 13/500\n",
      "1000/1000 [==============================] - 0s 125us/step - loss: 1.5964 - val_loss: 1.5436\n",
      "Epoch 14/500\n",
      "1000/1000 [==============================] - 0s 126us/step - loss: 1.4759 - val_loss: 1.4356\n",
      "Epoch 15/500\n",
      "1000/1000 [==============================] - 0s 129us/step - loss: 1.3367 - val_loss: 1.4018\n",
      "Epoch 16/500\n",
      "1000/1000 [==============================] - 0s 129us/step - loss: 1.2839 - val_loss: 1.2601\n",
      "Epoch 17/500\n",
      "1000/1000 [==============================] - 0s 127us/step - loss: 1.2557 - val_loss: 1.2105\n",
      "Epoch 18/500\n",
      "1000/1000 [==============================] - 0s 129us/step - loss: 1.1659 - val_loss: 1.1905\n",
      "Epoch 19/500\n",
      "1000/1000 [==============================] - 0s 126us/step - loss: 1.0918 - val_loss: 1.0674\n",
      "Epoch 20/500\n",
      "1000/1000 [==============================] - 0s 133us/step - loss: 1.0121 - val_loss: 1.0075\n",
      "Epoch 21/500\n",
      "1000/1000 [==============================] - 0s 168us/step - loss: 0.9636 - val_loss: 0.9818\n",
      "Epoch 22/500\n",
      "1000/1000 [==============================] - 0s 166us/step - loss: 0.8864 - val_loss: 0.8694\n",
      "Epoch 23/500\n",
      "1000/1000 [==============================] - 0s 149us/step - loss: 0.8877 - val_loss: 0.8784\n",
      "Epoch 24/500\n",
      "1000/1000 [==============================] - 0s 151us/step - loss: 0.8378 - val_loss: 0.8139\n",
      "Epoch 25/500\n",
      "1000/1000 [==============================] - 0s 148us/step - loss: 0.7824 - val_loss: 0.7971\n",
      "Epoch 26/500\n",
      "1000/1000 [==============================] - 0s 135us/step - loss: 0.7120 - val_loss: 0.7387\n",
      "Epoch 27/500\n",
      "1000/1000 [==============================] - 0s 144us/step - loss: 0.7063 - val_loss: 0.7234\n",
      "Epoch 28/500\n",
      "1000/1000 [==============================] - 0s 133us/step - loss: 0.6585 - val_loss: 0.7025\n",
      "Epoch 29/500\n",
      "1000/1000 [==============================] - 0s 157us/step - loss: 0.6601 - val_loss: 0.6267\n",
      "Epoch 30/500\n",
      "1000/1000 [==============================] - 0s 147us/step - loss: 0.6330 - val_loss: 0.6056\n",
      "Epoch 31/500\n",
      "1000/1000 [==============================] - 0s 151us/step - loss: 0.5727 - val_loss: 0.5647\n",
      "Epoch 32/500\n",
      "1000/1000 [==============================] - 0s 142us/step - loss: 0.5369 - val_loss: 0.5507\n",
      "Epoch 33/500\n",
      "1000/1000 [==============================] - 0s 128us/step - loss: 0.5252 - val_loss: 0.5059\n",
      "Epoch 34/500\n",
      "1000/1000 [==============================] - 0s 127us/step - loss: 0.4866 - val_loss: 0.4869\n",
      "Epoch 35/500\n",
      "1000/1000 [==============================] - 0s 152us/step - loss: 0.4911 - val_loss: 0.4904\n",
      "Epoch 36/500\n",
      "1000/1000 [==============================] - 0s 149us/step - loss: 0.4511 - val_loss: 0.4703\n",
      "Epoch 37/500\n",
      "1000/1000 [==============================] - 0s 138us/step - loss: 0.4433 - val_loss: 0.4389\n",
      "Epoch 38/500\n",
      "1000/1000 [==============================] - 0s 139us/step - loss: 0.4130 - val_loss: 0.4065\n",
      "Epoch 39/500\n",
      "1000/1000 [==============================] - 0s 161us/step - loss: 0.3731 - val_loss: 0.3859\n",
      "Epoch 40/500\n",
      "1000/1000 [==============================] - 0s 157us/step - loss: 0.3804 - val_loss: 0.3728\n",
      "Epoch 41/500\n",
      "1000/1000 [==============================] - 0s 169us/step - loss: 0.3657 - val_loss: 0.3607\n",
      "Epoch 42/500\n",
      "1000/1000 [==============================] - 0s 193us/step - loss: 0.3318 - val_loss: 0.3496\n",
      "Epoch 43/500\n",
      "1000/1000 [==============================] - 0s 154us/step - loss: 0.3361 - val_loss: 0.3187\n",
      "Epoch 44/500\n",
      "1000/1000 [==============================] - 0s 223us/step - loss: 0.3070 - val_loss: 0.3193\n",
      "Epoch 45/500\n",
      "1000/1000 [==============================] - 0s 140us/step - loss: 0.2765 - val_loss: 0.3183\n",
      "Epoch 46/500\n",
      "1000/1000 [==============================] - 0s 121us/step - loss: 0.2803 - val_loss: 0.2793\n",
      "Epoch 47/500\n",
      "1000/1000 [==============================] - 0s 160us/step - loss: 0.2698 - val_loss: 0.2695\n",
      "Epoch 48/500\n",
      "1000/1000 [==============================] - 0s 140us/step - loss: 0.2559 - val_loss: 0.2505\n",
      "Epoch 49/500\n",
      "1000/1000 [==============================] - 0s 126us/step - loss: 0.2579 - val_loss: 0.2558\n",
      "Epoch 50/500\n",
      "1000/1000 [==============================] - 0s 113us/step - loss: 0.2406 - val_loss: 0.2611\n",
      "Epoch 51/500\n",
      "1000/1000 [==============================] - 0s 132us/step - loss: 0.2288 - val_loss: 0.2167\n",
      "Epoch 52/500\n",
      "1000/1000 [==============================] - 0s 123us/step - loss: 0.2137 - val_loss: 0.2236\n",
      "Epoch 53/500\n",
      "1000/1000 [==============================] - 0s 118us/step - loss: 0.2186 - val_loss: 0.2253\n",
      "Epoch 54/500\n",
      "1000/1000 [==============================] - 0s 154us/step - loss: 0.2021 - val_loss: 0.1896\n",
      "Epoch 55/500\n",
      "1000/1000 [==============================] - 0s 130us/step - loss: 0.1877 - val_loss: 0.2059\n",
      "Epoch 56/500\n",
      "1000/1000 [==============================] - 0s 126us/step - loss: 0.1984 - val_loss: 0.1881\n",
      "Epoch 57/500\n",
      "1000/1000 [==============================] - 0s 124us/step - loss: 0.1791 - val_loss: 0.1800\n",
      "Epoch 58/500\n",
      "1000/1000 [==============================] - 0s 124us/step - loss: 0.1632 - val_loss: 0.1625\n",
      "Epoch 59/500\n",
      "1000/1000 [==============================] - 0s 122us/step - loss: 0.1610 - val_loss: 0.1721\n",
      "Epoch 60/500\n",
      "1000/1000 [==============================] - 0s 147us/step - loss: 0.1621 - val_loss: 0.1863\n",
      "Epoch 61/500\n",
      "1000/1000 [==============================] - 0s 126us/step - loss: 0.1663 - val_loss: 0.1552\n",
      "Epoch 62/500\n",
      "1000/1000 [==============================] - 0s 125us/step - loss: 0.1444 - val_loss: 0.1516\n",
      "Epoch 63/500\n",
      "1000/1000 [==============================] - 0s 136us/step - loss: 0.1454 - val_loss: 0.1440\n",
      "Epoch 64/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 181us/step - loss: 0.1272 - val_loss: 0.1583\n",
      "Epoch 65/500\n",
      "1000/1000 [==============================] - 0s 162us/step - loss: 0.1503 - val_loss: 0.1300\n",
      "Epoch 66/500\n",
      "1000/1000 [==============================] - 0s 138us/step - loss: 0.1259 - val_loss: 0.1364\n",
      "Epoch 67/500\n",
      "1000/1000 [==============================] - 0s 129us/step - loss: 0.1271 - val_loss: 0.1385\n",
      "Epoch 68/500\n",
      "1000/1000 [==============================] - 0s 115us/step - loss: 0.1193 - val_loss: 0.1240\n",
      "Epoch 69/500\n",
      "1000/1000 [==============================] - 0s 123us/step - loss: 0.1201 - val_loss: 0.1193\n",
      "Epoch 70/500\n",
      "1000/1000 [==============================] - 0s 131us/step - loss: 0.1110 - val_loss: 0.1349\n",
      "Epoch 71/500\n",
      "1000/1000 [==============================] - 0s 132us/step - loss: 0.1012 - val_loss: 0.1084\n",
      "Epoch 72/500\n",
      "1000/1000 [==============================] - 0s 128us/step - loss: 0.1083 - val_loss: 0.1214\n",
      "Epoch 73/500\n",
      "1000/1000 [==============================] - 0s 147us/step - loss: 0.1114 - val_loss: 0.1187\n",
      "Epoch 74/500\n",
      "1000/1000 [==============================] - 0s 108us/step - loss: 0.1061 - val_loss: 0.1128\n",
      "Epoch 75/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0883 - val_loss: 0.1080\n",
      "Epoch 76/500\n",
      "1000/1000 [==============================] - 0s 107us/step - loss: 0.1203 - val_loss: 0.1165\n",
      "Epoch 77/500\n",
      "1000/1000 [==============================] - 0s 115us/step - loss: 0.1005 - val_loss: 0.1081\n",
      "Epoch 78/500\n",
      "1000/1000 [==============================] - 0s 128us/step - loss: 0.0978 - val_loss: 0.1036\n",
      "Epoch 79/500\n",
      "1000/1000 [==============================] - 0s 113us/step - loss: 0.1010 - val_loss: 0.1071\n",
      "Epoch 80/500\n",
      "1000/1000 [==============================] - 0s 109us/step - loss: 0.0980 - val_loss: 0.1018\n",
      "Epoch 81/500\n",
      "1000/1000 [==============================] - 0s 108us/step - loss: 0.0987 - val_loss: 0.1087\n",
      "Epoch 82/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0761 - val_loss: 0.1067\n",
      "Epoch 83/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0953 - val_loss: 0.1089\n",
      "Epoch 84/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0909 - val_loss: 0.0958\n",
      "Epoch 85/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0841 - val_loss: 0.0949\n",
      "Epoch 86/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0789 - val_loss: 0.0917\n",
      "Epoch 87/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0935 - val_loss: 0.1016\n",
      "Epoch 88/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.1030 - val_loss: 0.0879\n",
      "Epoch 89/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0939 - val_loss: 0.0946\n",
      "Epoch 90/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0836 - val_loss: 0.0732\n",
      "Epoch 91/500\n",
      "1000/1000 [==============================] - 0s 109us/step - loss: 0.0924 - val_loss: 0.1048\n",
      "Epoch 92/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0835 - val_loss: 0.0916\n",
      "Epoch 93/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0931 - val_loss: 0.0874\n",
      "Epoch 94/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0860 - val_loss: 0.0865\n",
      "Epoch 95/500\n",
      "1000/1000 [==============================] - 0s 107us/step - loss: 0.0930 - val_loss: 0.0874\n",
      "Epoch 96/500\n",
      "1000/1000 [==============================] - 0s 108us/step - loss: 0.0819 - val_loss: 0.0893\n",
      "Epoch 97/500\n",
      "1000/1000 [==============================] - 0s 108us/step - loss: 0.0831 - val_loss: 0.1030\n",
      "Epoch 98/500\n",
      "1000/1000 [==============================] - 0s 107us/step - loss: 0.0842 - val_loss: 0.1056\n",
      "Epoch 99/500\n",
      "1000/1000 [==============================] - 0s 108us/step - loss: 0.0682 - val_loss: 0.0772\n",
      "Epoch 100/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0849 - val_loss: 0.0970\n",
      "Epoch 101/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.1016 - val_loss: 0.0961\n",
      "Epoch 102/500\n",
      "1000/1000 [==============================] - 0s 108us/step - loss: 0.0638 - val_loss: 0.0916\n",
      "Epoch 103/500\n",
      "1000/1000 [==============================] - 0s 110us/step - loss: 0.0787 - val_loss: 0.0906\n",
      "Epoch 104/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0880 - val_loss: 0.0791\n",
      "Epoch 105/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0939 - val_loss: 0.0922\n",
      "Epoch 106/500\n",
      "1000/1000 [==============================] - 0s 107us/step - loss: 0.0885 - val_loss: 0.1078\n",
      "Epoch 107/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0767 - val_loss: 0.1046\n",
      "Epoch 108/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0765 - val_loss: 0.1059\n",
      "Epoch 109/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0718 - val_loss: 0.0761\n",
      "Epoch 110/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0721 - val_loss: 0.0735\n",
      "Epoch 111/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0875 - val_loss: 0.0841\n",
      "Epoch 112/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0853 - val_loss: 0.0793\n",
      "Epoch 113/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0675 - val_loss: 0.0894\n",
      "Epoch 114/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0834 - val_loss: 0.0869\n",
      "Epoch 115/500\n",
      "1000/1000 [==============================] - 0s 110us/step - loss: 0.0929 - val_loss: 0.1006\n",
      "Epoch 116/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0653 - val_loss: 0.0781\n",
      "Epoch 117/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0853 - val_loss: 0.0704\n",
      "Epoch 118/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0651 - val_loss: 0.0873\n",
      "Epoch 119/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0853 - val_loss: 0.0929\n",
      "Epoch 120/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0638 - val_loss: 0.0887\n",
      "Epoch 121/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0850 - val_loss: 0.0842\n",
      "Epoch 122/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0836 - val_loss: 0.0959\n",
      "Epoch 123/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0809 - val_loss: 0.1191\n",
      "Epoch 124/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0887 - val_loss: 0.0976\n",
      "Epoch 125/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0704 - val_loss: 0.0810\n",
      "Epoch 126/500\n",
      "1000/1000 [==============================] - 0s 135us/step - loss: 0.0960 - val_loss: 0.0918\n",
      "Epoch 127/500\n",
      "1000/1000 [==============================] - 0s 111us/step - loss: 0.0909 - val_loss: 0.0857\n",
      "Epoch 128/500\n",
      "1000/1000 [==============================] - 0s 114us/step - loss: 0.0902 - val_loss: 0.0772\n",
      "Epoch 129/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0778 - val_loss: 0.0778\n",
      "Epoch 130/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0739 - val_loss: 0.0857\n",
      "Epoch 131/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0884 - val_loss: 0.1014\n",
      "Epoch 132/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0794 - val_loss: 0.0783\n",
      "Epoch 133/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0770 - val_loss: 0.0983\n",
      "Epoch 134/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0849 - val_loss: 0.0984\n",
      "Epoch 135/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0726 - val_loss: 0.0897\n",
      "Epoch 136/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0850 - val_loss: 0.0837\n",
      "Epoch 137/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0863 - val_loss: 0.0938\n",
      "Epoch 138/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0799 - val_loss: 0.0874\n",
      "Epoch 139/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0817 - val_loss: 0.0959\n",
      "Epoch 140/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0777 - val_loss: 0.0723\n",
      "Epoch 141/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 113us/step - loss: 0.0842 - val_loss: 0.0891\n",
      "Epoch 142/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0763 - val_loss: 0.0941\n",
      "Epoch 143/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0803 - val_loss: 0.0816\n",
      "Epoch 144/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0669 - val_loss: 0.0894\n",
      "Epoch 145/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0694 - val_loss: 0.0800\n",
      "Epoch 146/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0836 - val_loss: 0.1083\n",
      "Epoch 147/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0688 - val_loss: 0.0917\n",
      "Epoch 148/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0700 - val_loss: 0.0962\n",
      "Epoch 149/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0769 - val_loss: 0.0862\n",
      "Epoch 150/500\n",
      "1000/1000 [==============================] - 0s 109us/step - loss: 0.0733 - val_loss: 0.0827\n",
      "Epoch 151/500\n",
      "1000/1000 [==============================] - 0s 102us/step - loss: 0.0666 - val_loss: 0.0978\n",
      "Epoch 152/500\n",
      "1000/1000 [==============================] - 0s 111us/step - loss: 0.0665 - val_loss: 0.0895\n",
      "Epoch 153/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0806 - val_loss: 0.0894\n",
      "Epoch 154/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0836 - val_loss: 0.0850\n",
      "Epoch 155/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0754 - val_loss: 0.0773\n",
      "Epoch 156/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0808 - val_loss: 0.0796\n",
      "Epoch 157/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0798 - val_loss: 0.0678\n",
      "Epoch 158/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0890 - val_loss: 0.0810\n",
      "Epoch 159/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0869 - val_loss: 0.0933\n",
      "Epoch 160/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0702 - val_loss: 0.0964\n",
      "Epoch 161/500\n",
      "1000/1000 [==============================] - 0s 107us/step - loss: 0.0868 - val_loss: 0.1047\n",
      "Epoch 162/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0873 - val_loss: 0.0950\n",
      "Epoch 163/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0816 - val_loss: 0.0761\n",
      "Epoch 164/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0692 - val_loss: 0.0855\n",
      "Epoch 165/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0670 - val_loss: 0.1000\n",
      "Epoch 166/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0890 - val_loss: 0.0902\n",
      "Epoch 167/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0769 - val_loss: 0.0794\n",
      "Epoch 168/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0741 - val_loss: 0.0930\n",
      "Epoch 169/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0773 - val_loss: 0.0956\n",
      "Epoch 170/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0739 - val_loss: 0.0908\n",
      "Epoch 171/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0719 - val_loss: 0.0835\n",
      "Epoch 172/500\n",
      "1000/1000 [==============================] - 0s 107us/step - loss: 0.0617 - val_loss: 0.0794\n",
      "Epoch 173/500\n",
      "1000/1000 [==============================] - 0s 107us/step - loss: 0.0714 - val_loss: 0.0928\n",
      "Epoch 174/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0759 - val_loss: 0.0970\n",
      "Epoch 175/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0737 - val_loss: 0.0894\n",
      "Epoch 176/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0797 - val_loss: 0.0858\n",
      "Epoch 177/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0770 - val_loss: 0.0912\n",
      "Epoch 178/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0786 - val_loss: 0.0819\n",
      "Epoch 179/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0582 - val_loss: 0.0928\n",
      "Epoch 180/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0835 - val_loss: 0.0868\n",
      "Epoch 181/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0849 - val_loss: 0.0796\n",
      "Epoch 182/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0680 - val_loss: 0.0847\n",
      "Epoch 183/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0754 - val_loss: 0.0831\n",
      "Epoch 184/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0794 - val_loss: 0.0786\n",
      "Epoch 185/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0810 - val_loss: 0.1115\n",
      "Epoch 186/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0580 - val_loss: 0.0927\n",
      "Epoch 187/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0586 - val_loss: 0.0899\n",
      "Epoch 188/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0693 - val_loss: 0.0721\n",
      "Epoch 189/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0921 - val_loss: 0.0786\n",
      "Epoch 190/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0770 - val_loss: 0.0774\n",
      "Epoch 191/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0833 - val_loss: 0.0830\n",
      "Epoch 192/500\n",
      "1000/1000 [==============================] - 0s 107us/step - loss: 0.0865 - val_loss: 0.0972\n",
      "Epoch 193/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0863 - val_loss: 0.0861\n",
      "Epoch 194/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0842 - val_loss: 0.0675\n",
      "Epoch 195/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0827 - val_loss: 0.0869\n",
      "Epoch 196/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0587 - val_loss: 0.0902\n",
      "Epoch 197/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0694 - val_loss: 0.0850\n",
      "Epoch 198/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0797 - val_loss: 0.0808\n",
      "Epoch 199/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0702 - val_loss: 0.0839\n",
      "Epoch 200/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0746 - val_loss: 0.0849\n",
      "Epoch 201/500\n",
      "1000/1000 [==============================] - 0s 113us/step - loss: 0.0677 - val_loss: 0.0715\n",
      "Epoch 202/500\n",
      "1000/1000 [==============================] - 0s 107us/step - loss: 0.0755 - val_loss: 0.0845\n",
      "Epoch 203/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0784 - val_loss: 0.0937\n",
      "Epoch 204/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0817 - val_loss: 0.0910\n",
      "Epoch 205/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0914 - val_loss: 0.0971\n",
      "Epoch 206/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0904 - val_loss: 0.0736\n",
      "Epoch 207/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0744 - val_loss: 0.0874\n",
      "Epoch 208/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0659 - val_loss: 0.0772\n",
      "Epoch 209/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0653 - val_loss: 0.0966\n",
      "Epoch 210/500\n",
      "1000/1000 [==============================] - 0s 107us/step - loss: 0.0702 - val_loss: 0.0922\n",
      "Epoch 211/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0719 - val_loss: 0.0691\n",
      "Epoch 212/500\n",
      "1000/1000 [==============================] - 0s 116us/step - loss: 0.0779 - val_loss: 0.0861\n",
      "Epoch 213/500\n",
      "1000/1000 [==============================] - 0s 109us/step - loss: 0.0751 - val_loss: 0.0870\n",
      "Epoch 214/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0755 - val_loss: 0.0963\n",
      "Epoch 215/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0811 - val_loss: 0.0904\n",
      "Epoch 216/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0618 - val_loss: 0.0849\n",
      "Epoch 217/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0645 - val_loss: 0.1005\n",
      "Epoch 218/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0912 - val_loss: 0.0900\n",
      "Epoch 219/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0766 - val_loss: 0.0944\n",
      "Epoch 220/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0672 - val_loss: 0.1040\n",
      "Epoch 221/500\n",
      "1000/1000 [==============================] - 0s 126us/step - loss: 0.0942 - val_loss: 0.0969\n",
      "Epoch 222/500\n",
      "1000/1000 [==============================] - 0s 123us/step - loss: 0.0830 - val_loss: 0.0749\n",
      "Epoch 223/500\n",
      "1000/1000 [==============================] - 0s 111us/step - loss: 0.0771 - val_loss: 0.0792\n",
      "Epoch 224/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0733 - val_loss: 0.0930\n",
      "Epoch 225/500\n",
      "1000/1000 [==============================] - 0s 108us/step - loss: 0.0684 - val_loss: 0.0959\n",
      "Epoch 226/500\n",
      "1000/1000 [==============================] - 0s 107us/step - loss: 0.0686 - val_loss: 0.0904\n",
      "Epoch 227/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0800 - val_loss: 0.0892\n",
      "Epoch 228/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0777 - val_loss: 0.0720\n",
      "Epoch 229/500\n",
      "1000/1000 [==============================] - 0s 108us/step - loss: 0.0807 - val_loss: 0.0974\n",
      "Epoch 230/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0750 - val_loss: 0.0899\n",
      "Epoch 231/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0804 - val_loss: 0.0829\n",
      "Epoch 232/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0780 - val_loss: 0.0886\n",
      "Epoch 233/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0844 - val_loss: 0.0696\n",
      "Epoch 234/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0790 - val_loss: 0.0793\n",
      "Epoch 235/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0825 - val_loss: 0.0758\n",
      "Epoch 236/500\n",
      "1000/1000 [==============================] - 0s 116us/step - loss: 0.0774 - val_loss: 0.0688\n",
      "Epoch 237/500\n",
      "1000/1000 [==============================] - 0s 108us/step - loss: 0.0832 - val_loss: 0.0865\n",
      "Epoch 238/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0616 - val_loss: 0.0828\n",
      "Epoch 239/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0695 - val_loss: 0.1025\n",
      "Epoch 240/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0730 - val_loss: 0.1007\n",
      "Epoch 241/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0840 - val_loss: 0.0936\n",
      "Epoch 242/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0789 - val_loss: 0.0858\n",
      "Epoch 243/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0670 - val_loss: 0.1019\n",
      "Epoch 244/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0751 - val_loss: 0.0736\n",
      "Epoch 245/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0772 - val_loss: 0.0927\n",
      "Epoch 246/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0651 - val_loss: 0.0934\n",
      "Epoch 247/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0561 - val_loss: 0.0803\n",
      "Epoch 248/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0841 - val_loss: 0.1033\n",
      "Epoch 249/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0526 - val_loss: 0.0902\n",
      "Epoch 250/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0874 - val_loss: 0.1018\n",
      "Epoch 251/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0615 - val_loss: 0.0871\n",
      "Epoch 252/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0749 - val_loss: 0.0755\n",
      "Epoch 253/500\n",
      "1000/1000 [==============================] - 0s 107us/step - loss: 0.0900 - val_loss: 0.0818\n",
      "Epoch 254/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0708 - val_loss: 0.0740\n",
      "Epoch 255/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0846 - val_loss: 0.0705\n",
      "Epoch 256/500\n",
      "1000/1000 [==============================] - 0s 112us/step - loss: 0.0758 - val_loss: 0.0886\n",
      "Epoch 257/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0853 - val_loss: 0.0806\n",
      "Epoch 258/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0812 - val_loss: 0.0691\n",
      "Epoch 259/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0856 - val_loss: 0.0878\n",
      "Epoch 260/500\n",
      "1000/1000 [==============================] - 0s 108us/step - loss: 0.0779 - val_loss: 0.0796\n",
      "Epoch 261/500\n",
      "1000/1000 [==============================] - 0s 107us/step - loss: 0.0829 - val_loss: 0.0907\n",
      "Epoch 262/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0578 - val_loss: 0.0829\n",
      "Epoch 263/500\n",
      "1000/1000 [==============================] - 0s 108us/step - loss: 0.0848 - val_loss: 0.0889\n",
      "Epoch 264/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0731 - val_loss: 0.0789\n",
      "Epoch 265/500\n",
      "1000/1000 [==============================] - 0s 112us/step - loss: 0.0867 - val_loss: 0.0859\n",
      "Epoch 266/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0844 - val_loss: 0.0809\n",
      "Epoch 267/500\n",
      "1000/1000 [==============================] - 0s 107us/step - loss: 0.0855 - val_loss: 0.0887\n",
      "Epoch 268/500\n",
      "1000/1000 [==============================] - 0s 109us/step - loss: 0.0749 - val_loss: 0.0636\n",
      "Epoch 269/500\n",
      "1000/1000 [==============================] - 0s 109us/step - loss: 0.0682 - val_loss: 0.0845\n",
      "Epoch 270/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0714 - val_loss: 0.0868\n",
      "Epoch 271/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0786 - val_loss: 0.0874\n",
      "Epoch 272/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0788 - val_loss: 0.0973\n",
      "Epoch 273/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0951 - val_loss: 0.0931\n",
      "Epoch 274/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0831 - val_loss: 0.0948\n",
      "Epoch 275/500\n",
      "1000/1000 [==============================] - 0s 107us/step - loss: 0.0775 - val_loss: 0.0821\n",
      "Epoch 276/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0740 - val_loss: 0.0884\n",
      "Epoch 277/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0823 - val_loss: 0.1014\n",
      "Epoch 278/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0619 - val_loss: 0.0930\n",
      "Epoch 279/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0760 - val_loss: 0.0929\n",
      "Epoch 280/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0824 - val_loss: 0.1023\n",
      "Epoch 281/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0804 - val_loss: 0.0999\n",
      "Epoch 282/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0614 - val_loss: 0.0888\n",
      "Epoch 283/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0600 - val_loss: 0.0920\n",
      "Epoch 284/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0725 - val_loss: 0.0767\n",
      "Epoch 285/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0906 - val_loss: 0.0588\n",
      "Epoch 286/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0920 - val_loss: 0.0796\n",
      "Epoch 287/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0748 - val_loss: 0.0737\n",
      "Epoch 288/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0726 - val_loss: 0.0821\n",
      "Epoch 289/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0702 - val_loss: 0.0869\n",
      "Epoch 290/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0851 - val_loss: 0.0854\n",
      "Epoch 291/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0783 - val_loss: 0.0854\n",
      "Epoch 292/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0748 - val_loss: 0.0828\n",
      "Epoch 293/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0646 - val_loss: 0.0860\n",
      "Epoch 294/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0623 - val_loss: 0.0712\n",
      "Epoch 295/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0697 - val_loss: 0.0919\n",
      "Epoch 296/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0784 - val_loss: 0.0829\n",
      "Epoch 297/500\n",
      "1000/1000 [==============================] - 0s 107us/step - loss: 0.0800 - val_loss: 0.0874\n",
      "Epoch 298/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0876 - val_loss: 0.0876\n",
      "Epoch 299/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0618 - val_loss: 0.0753\n",
      "Epoch 300/500\n",
      "1000/1000 [==============================] - 0s 107us/step - loss: 0.0671 - val_loss: 0.0902\n",
      "Epoch 301/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0870 - val_loss: 0.0837\n",
      "Epoch 302/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0783 - val_loss: 0.0861\n",
      "Epoch 303/500\n",
      "1000/1000 [==============================] - 0s 113us/step - loss: 0.0773 - val_loss: 0.0827\n",
      "Epoch 304/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0658 - val_loss: 0.0870\n",
      "Epoch 305/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0821 - val_loss: 0.0911\n",
      "Epoch 306/500\n",
      "1000/1000 [==============================] - 0s 107us/step - loss: 0.0670 - val_loss: 0.0677\n",
      "Epoch 307/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0752 - val_loss: 0.0834\n",
      "Epoch 308/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0678 - val_loss: 0.0725\n",
      "Epoch 309/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0882 - val_loss: 0.0893\n",
      "Epoch 310/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0884 - val_loss: 0.0662\n",
      "Epoch 311/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0642 - val_loss: 0.0804\n",
      "Epoch 312/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0771 - val_loss: 0.0866\n",
      "Epoch 313/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0861 - val_loss: 0.0928\n",
      "Epoch 314/500\n",
      "1000/1000 [==============================] - 0s 109us/step - loss: 0.0775 - val_loss: 0.0833\n",
      "Epoch 315/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0729 - val_loss: 0.0864\n",
      "Epoch 316/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0895 - val_loss: 0.0973\n",
      "Epoch 317/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0914 - val_loss: 0.0969\n",
      "Epoch 318/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0627 - val_loss: 0.0875\n",
      "Epoch 319/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0819 - val_loss: 0.0698\n",
      "Epoch 320/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0588 - val_loss: 0.0873\n",
      "Epoch 321/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0830 - val_loss: 0.0885\n",
      "Epoch 322/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0811 - val_loss: 0.0727\n",
      "Epoch 323/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0750 - val_loss: 0.0721\n",
      "Epoch 324/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0866 - val_loss: 0.0971\n",
      "Epoch 325/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0790 - val_loss: 0.0654\n",
      "Epoch 326/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0735 - val_loss: 0.0797\n",
      "Epoch 327/500\n",
      "1000/1000 [==============================] - 0s 107us/step - loss: 0.0655 - val_loss: 0.0852\n",
      "Epoch 328/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0635 - val_loss: 0.0762\n",
      "Epoch 329/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0699 - val_loss: 0.0826\n",
      "Epoch 330/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0717 - val_loss: 0.0863\n",
      "Epoch 331/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0892 - val_loss: 0.0792\n",
      "Epoch 332/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0696 - val_loss: 0.0898\n",
      "Epoch 333/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0735 - val_loss: 0.0980\n",
      "Epoch 334/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0678 - val_loss: 0.0776\n",
      "Epoch 335/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0759 - val_loss: 0.0877\n",
      "Epoch 336/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0957 - val_loss: 0.0827\n",
      "Epoch 337/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0729 - val_loss: 0.1026\n",
      "Epoch 338/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0738 - val_loss: 0.0785\n",
      "Epoch 339/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0727 - val_loss: 0.0814\n",
      "Epoch 340/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0842 - val_loss: 0.0705\n",
      "Epoch 341/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0746 - val_loss: 0.0732\n",
      "Epoch 342/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0949 - val_loss: 0.0759\n",
      "Epoch 343/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0730 - val_loss: 0.0866\n",
      "Epoch 344/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0638 - val_loss: 0.0857\n",
      "Epoch 345/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0740 - val_loss: 0.0856\n",
      "Epoch 346/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0708 - val_loss: 0.0846\n",
      "Epoch 347/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0839 - val_loss: 0.0699\n",
      "Epoch 348/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0691 - val_loss: 0.0804\n",
      "Epoch 349/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0983 - val_loss: 0.0782\n",
      "Epoch 350/500\n",
      "1000/1000 [==============================] - 0s 111us/step - loss: 0.0803 - val_loss: 0.0761\n",
      "Epoch 351/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0848 - val_loss: 0.0787\n",
      "Epoch 352/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0733 - val_loss: 0.0684\n",
      "Epoch 353/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0616 - val_loss: 0.0653\n",
      "Epoch 354/500\n",
      "1000/1000 [==============================] - 0s 102us/step - loss: 0.0793 - val_loss: 0.0805\n",
      "Epoch 355/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0651 - val_loss: 0.0848\n",
      "Epoch 356/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0729 - val_loss: 0.0825\n",
      "Epoch 357/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0704 - val_loss: 0.0827\n",
      "Epoch 358/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0922 - val_loss: 0.0942\n",
      "Epoch 359/500\n",
      "1000/1000 [==============================] - 0s 112us/step - loss: 0.0727 - val_loss: 0.0890\n",
      "Epoch 360/500\n",
      "1000/1000 [==============================] - 0s 116us/step - loss: 0.0634 - val_loss: 0.0828\n",
      "Epoch 361/500\n",
      "1000/1000 [==============================] - 0s 111us/step - loss: 0.0538 - val_loss: 0.0860\n",
      "Epoch 362/500\n",
      "1000/1000 [==============================] - 0s 107us/step - loss: 0.0712 - val_loss: 0.0989\n",
      "Epoch 363/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0721 - val_loss: 0.0902\n",
      "Epoch 364/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0804 - val_loss: 0.0903\n",
      "Epoch 365/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0700 - val_loss: 0.0948\n",
      "Epoch 366/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0729 - val_loss: 0.0960\n",
      "Epoch 367/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0750 - val_loss: 0.0781\n",
      "Epoch 368/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0719 - val_loss: 0.0771\n",
      "Epoch 369/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0863 - val_loss: 0.0806\n",
      "Epoch 370/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0666 - val_loss: 0.0868\n",
      "Epoch 371/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0759 - val_loss: 0.0767\n",
      "Epoch 372/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0707 - val_loss: 0.0646\n",
      "Epoch 373/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0661 - val_loss: 0.0717\n",
      "Epoch 374/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0651 - val_loss: 0.0848\n",
      "Epoch 375/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0880 - val_loss: 0.0957\n",
      "Epoch 376/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0802 - val_loss: 0.0875\n",
      "Epoch 377/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0678 - val_loss: 0.0862\n",
      "Epoch 378/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0731 - val_loss: 0.0802\n",
      "Epoch 379/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0773 - val_loss: 0.0823\n",
      "Epoch 380/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0857 - val_loss: 0.0826\n",
      "Epoch 381/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0661 - val_loss: 0.0940\n",
      "Epoch 382/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0600 - val_loss: 0.0765\n",
      "Epoch 383/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0755 - val_loss: 0.0746\n",
      "Epoch 384/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0723 - val_loss: 0.0841\n",
      "Epoch 385/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0518 - val_loss: 0.1016\n",
      "Epoch 386/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0815 - val_loss: 0.0779\n",
      "Epoch 387/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0776 - val_loss: 0.0879\n",
      "Epoch 388/500\n",
      "1000/1000 [==============================] - 0s 111us/step - loss: 0.0776 - val_loss: 0.0773\n",
      "Epoch 389/500\n",
      "1000/1000 [==============================] - 0s 107us/step - loss: 0.0819 - val_loss: 0.0797\n",
      "Epoch 390/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0816 - val_loss: 0.0884\n",
      "Epoch 391/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0722 - val_loss: 0.0765\n",
      "Epoch 392/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0762 - val_loss: 0.0909\n",
      "Epoch 393/500\n",
      "1000/1000 [==============================] - 0s 109us/step - loss: 0.0763 - val_loss: 0.0647\n",
      "Epoch 394/500\n",
      "1000/1000 [==============================] - 0s 109us/step - loss: 0.0614 - val_loss: 0.0677\n",
      "Epoch 395/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0776 - val_loss: 0.0715\n",
      "Epoch 396/500\n",
      "1000/1000 [==============================] - 0s 102us/step - loss: 0.0783 - val_loss: 0.0612\n",
      "Epoch 397/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0725 - val_loss: 0.0878\n",
      "Epoch 398/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0750 - val_loss: 0.0814\n",
      "Epoch 399/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0823 - val_loss: 0.0823\n",
      "Epoch 400/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0603 - val_loss: 0.0795\n",
      "Epoch 401/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0588 - val_loss: 0.0907\n",
      "Epoch 402/500\n",
      "1000/1000 [==============================] - 0s 108us/step - loss: 0.0796 - val_loss: 0.0899\n",
      "Epoch 403/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0674 - val_loss: 0.0838\n",
      "Epoch 404/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0714 - val_loss: 0.0723\n",
      "Epoch 405/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0697 - val_loss: 0.0837\n",
      "Epoch 406/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0779 - val_loss: 0.0689\n",
      "Epoch 407/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0608 - val_loss: 0.0761\n",
      "Epoch 408/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0737 - val_loss: 0.0651\n",
      "Epoch 409/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0791 - val_loss: 0.0670\n",
      "Epoch 410/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0825 - val_loss: 0.0926\n",
      "Epoch 411/500\n",
      "1000/1000 [==============================] - 0s 107us/step - loss: 0.0676 - val_loss: 0.0704\n",
      "Epoch 412/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0780 - val_loss: 0.0898\n",
      "Epoch 413/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0839 - val_loss: 0.0825\n",
      "Epoch 414/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0627 - val_loss: 0.0800\n",
      "Epoch 415/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0697 - val_loss: 0.0833\n",
      "Epoch 416/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0798 - val_loss: 0.1064\n",
      "Epoch 417/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0721 - val_loss: 0.0718\n",
      "Epoch 418/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0774 - val_loss: 0.0775\n",
      "Epoch 419/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0720 - val_loss: 0.0790\n",
      "Epoch 420/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0679 - val_loss: 0.0637\n",
      "Epoch 421/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0652 - val_loss: 0.0786\n",
      "Epoch 422/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0829 - val_loss: 0.0788\n",
      "Epoch 423/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0824 - val_loss: 0.0863\n",
      "Epoch 424/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0761 - val_loss: 0.0851\n",
      "Epoch 425/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0829 - val_loss: 0.0818\n",
      "Epoch 426/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0742 - val_loss: 0.0711\n",
      "Epoch 427/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0467 - val_loss: 0.0966\n",
      "Epoch 428/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0680 - val_loss: 0.0767\n",
      "Epoch 429/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0745 - val_loss: 0.0796\n",
      "Epoch 430/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0697 - val_loss: 0.0722\n",
      "Epoch 431/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0912 - val_loss: 0.0723\n",
      "Epoch 432/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0627 - val_loss: 0.0887\n",
      "Epoch 433/500\n",
      "1000/1000 [==============================] - 0s 102us/step - loss: 0.0737 - val_loss: 0.0764\n",
      "Epoch 434/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0654 - val_loss: 0.0758\n",
      "Epoch 435/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0539 - val_loss: 0.0793\n",
      "Epoch 436/500\n",
      "1000/1000 [==============================] - 0s 111us/step - loss: 0.0856 - val_loss: 0.0826\n",
      "Epoch 437/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0696 - val_loss: 0.0829\n",
      "Epoch 438/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0773 - val_loss: 0.0724\n",
      "Epoch 439/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0632 - val_loss: 0.0930\n",
      "Epoch 440/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0697 - val_loss: 0.0836\n",
      "Epoch 441/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0698 - val_loss: 0.0855\n",
      "Epoch 442/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0765 - val_loss: 0.0818\n",
      "Epoch 443/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0729 - val_loss: 0.0788\n",
      "Epoch 444/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0719 - val_loss: 0.0893\n",
      "Epoch 445/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0904 - val_loss: 0.0821\n",
      "Epoch 446/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0971 - val_loss: 0.0695\n",
      "Epoch 447/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0671 - val_loss: 0.0715\n",
      "Epoch 448/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0722 - val_loss: 0.0863\n",
      "Epoch 449/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0626 - val_loss: 0.0820\n",
      "Epoch 450/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0762 - val_loss: 0.0860\n",
      "Epoch 451/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0758 - val_loss: 0.0821\n",
      "Epoch 452/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0694 - val_loss: 0.0832\n",
      "Epoch 453/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0806 - val_loss: 0.0587\n",
      "Epoch 454/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0651 - val_loss: 0.0824\n",
      "Epoch 455/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0753 - val_loss: 0.0787\n",
      "Epoch 456/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0650 - val_loss: 0.0782\n",
      "Epoch 457/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0821 - val_loss: 0.0755\n",
      "Epoch 458/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0810 - val_loss: 0.0811\n",
      "Epoch 459/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0774 - val_loss: 0.0794\n",
      "Epoch 460/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0699 - val_loss: 0.0630\n",
      "Epoch 461/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0706 - val_loss: 0.0801\n",
      "Epoch 462/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0705 - val_loss: 0.0768\n",
      "Epoch 463/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0780 - val_loss: 0.0632\n",
      "Epoch 464/500\n",
      "1000/1000 [==============================] - 0s 102us/step - loss: 0.0757 - val_loss: 0.0779\n",
      "Epoch 465/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0718 - val_loss: 0.0612\n",
      "Epoch 466/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0783 - val_loss: 0.0900\n",
      "Epoch 467/500\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0537 - val_loss: 0.0757\n",
      "Epoch 468/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0892 - val_loss: 0.0785\n",
      "Epoch 469/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0800 - val_loss: 0.0743\n",
      "Epoch 470/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0891 - val_loss: 0.0788\n",
      "Epoch 471/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0735 - val_loss: 0.0908\n",
      "Epoch 472/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0688 - val_loss: 0.0808\n",
      "Epoch 473/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0738 - val_loss: 0.0785\n",
      "Epoch 474/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0877 - val_loss: 0.0844\n",
      "Epoch 475/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0922 - val_loss: 0.0810\n",
      "Epoch 476/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0750 - val_loss: 0.0722\n",
      "Epoch 477/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0747 - val_loss: 0.0814\n",
      "Epoch 478/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0807 - val_loss: 0.0495\n",
      "Epoch 479/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0587 - val_loss: 0.0861\n",
      "Epoch 480/500\n",
      "1000/1000 [==============================] - 0s 102us/step - loss: 0.0781 - val_loss: 0.1033\n",
      "Epoch 481/500\n",
      "1000/1000 [==============================] - 0s 102us/step - loss: 0.0850 - val_loss: 0.0875\n",
      "Epoch 482/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0639 - val_loss: 0.0823\n",
      "Epoch 483/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0684 - val_loss: 0.0815\n",
      "Epoch 484/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0697 - val_loss: 0.0794\n",
      "Epoch 485/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0730 - val_loss: 0.0744\n",
      "Epoch 486/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0757 - val_loss: 0.0734\n",
      "Epoch 487/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0816 - val_loss: 0.0696\n",
      "Epoch 488/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0682 - val_loss: 0.0937\n",
      "Epoch 489/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0746 - val_loss: 0.0837\n",
      "Epoch 490/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0646 - val_loss: 0.0939\n",
      "Epoch 491/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0659 - val_loss: 0.0789\n",
      "Epoch 492/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0696 - val_loss: 0.0744\n",
      "Epoch 493/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0675 - val_loss: 0.0884\n",
      "Epoch 494/500\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0750 - val_loss: 0.0879\n",
      "Epoch 495/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0591 - val_loss: 0.0680\n",
      "Epoch 496/500\n",
      "1000/1000 [==============================] - 0s 111us/step - loss: 0.0531 - val_loss: 0.0972\n",
      "Epoch 497/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0512 - val_loss: 0.0892\n",
      "Epoch 498/500\n",
      "1000/1000 [==============================] - 0s 105us/step - loss: 0.0582 - val_loss: 0.0822\n",
      "Epoch 499/500\n",
      "1000/1000 [==============================] - 0s 102us/step - loss: 0.0597 - val_loss: 0.0737\n",
      "Epoch 500/500\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0743 - val_loss: 0.0714\n"
     ]
    }
   ],
   "source": [
    "#if __name__ == '__main__':\n",
    "#    parser = argparse.ArgumentParser()\n",
    "#    help_ = \"Load h5 model trained weights\"\n",
    "#    parser.add_argument(\"-w\", \"--weights\", help=help_)\n",
    "#    help_ = \"Use mse loss instead of binary cross entropy (default)\"\n",
    "#    parser.add_argument(\"-m\",\n",
    "#                        \"--mse\",\n",
    "#                        help=help_, action='store_true')\n",
    "#    args = parser.parse_args()\n",
    "dummy = np.ones((1000,2))\n",
    "\n",
    "models = (encoder, decoder)\n",
    "    #data = (x_test, y_test)\n",
    "\n",
    "    # VAE loss = mse_loss or xent_loss + kl_loss\n",
    "#    if args.mse:\n",
    "reconstruction_loss_1 = mse(inputs, outputs_1)\n",
    "reconstruction_loss_2 = mse(inputs, outputs_2)\n",
    "#    else:\n",
    "#        reconstruction_loss = binary_crossentropy(inputs,\n",
    "#                                                  outputs)\n",
    "\n",
    "reconstruction_loss_1 *= original_dim * c\n",
    "reconstruction_loss_2 *= original_dim * (1-c)\n",
    "\n",
    "kl_loss_1 = 1 + z_log_var_1 - K.square(z_mean_1 - mu1) - K.exp(z_log_var_1)\n",
    "kl_loss_1 = K.sum(kl_loss_1, axis=-1)\n",
    "kl_loss_1 *= -0.5 * c\n",
    "\n",
    "kl_loss_2 = 1 + z_log_var_2 - K.square(z_mean_2 - mu2) - K.exp(z_log_var_2)\n",
    "kl_loss_2 = K.sum(kl_loss_2, axis=-1)\n",
    "kl_loss_2 *= -0.5 * (1-c)\n",
    "\n",
    "vae_loss = K.mean(reconstruction_loss_1 + reconstruction_loss_2 + kl_loss_1 + kl_loss_2 + c * K.log(c) + (1-c) * K.log(1-c)) \n",
    "\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    "vae.summary()\n",
    "#    plot_model(vae,\n",
    "#               to_file='vae_mlp.png',\n",
    "#               show_shapes=True)\n",
    "\n",
    "#    if args.weights:\n",
    "#        vae.load_weights(args.weights)\n",
    "#    else:\n",
    "        # train the autoencoder\n",
    "vae.fit([x_train,dummy],\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=([x_test,dummy], None))\n",
    "vae.save_weights('vae_mlp_mnist.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x183ce3e4a8>"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18449ab2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "[z_mean_1, z_log_var_1, z_mean_2, z_log_var_2, z_1, z_2, predict_c, mu1, mu2] = encoder.predict([x_test,dummy],batch_size=batch_size)\n",
    "color = np.ones(1000)\n",
    "for i in range(0, 1000):\n",
    "    if predict_c[i] < 0.5:\n",
    "        z_mean_1[i,:] = z_mean_2[i,:]\n",
    "        color[i] = 0\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(z_mean_1[:, 0], z_mean_1[:, 1], c=color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
