{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wuzhejun/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.layers import Lambda, Input, Dense, Reshape\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# reparameterization trick\n",
    "# instead of sampling from Q(z|X), sample eps = N(0,I)\n",
    "# z = z_mean + sqrt(var)*eps\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "    # Arguments:\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    # Returns:\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    thre = K.random_uniform(shape=(batch,1))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 784)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_intermediate (Dense)    (None, 512)          401920      encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "encoder_intermediate_2 (Dense)  (None, 16)           8208        encoder_intermediate[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dummy (InputLayer)              (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 3)            51          encoder_intermediate_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 3)            51          encoder_intermediate_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mu_vector (Dense)               (None, 9)            9           dummy[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "encoder_intermediate_3 (Dense)  (None, 16)           8208        encoder_intermediate[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (None, 3)            0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mu (Reshape)                    (None, 3, 3)         0           mu_vector[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "c (Dense)                       (None, 3)            51          encoder_intermediate_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "c_outlier (Dense)               (None, 2)            34          encoder_intermediate_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "pi (Dense)                      (None, 3)            6           dummy[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 418,538\n",
      "Trainable params: 418,538\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z_sampling (InputLayer)      (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               8704      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 784)               402192    \n",
      "=================================================================\n",
      "Total params: 410,960\n",
      "Trainable params: 410,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 784)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dummy (InputLayer)              (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Model)                 [(None, 3), (None, 3 418538      encoder_input[0][0]              \n",
      "                                                                 dummy[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Model)                 (None, 784)          410960      encoder[1][2]                    \n",
      "==================================================================================================\n",
      "Total params: 829,498\n",
      "Trainable params: 829,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "image_size = x_train.shape[1]\n",
    "original_dim = image_size * image_size\n",
    "x_train = np.reshape(x_train, [-1, original_dim])\n",
    "x_test = np.reshape(x_test, [-1, original_dim])\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "idx_train = (y_train == 0) |(y_train == 5) | (y_train == 7) | (np.random.rand(y_train.shape[0]) < 0.05)\n",
    "x_train = x_train[idx_train,:]\n",
    "y_train = y_train[idx_train]\n",
    "idx_test = (y_test == 0) |(y_test == 5) | (y_test == 7) | (np.random.rand(y_test.shape[0]) < 0.05)\n",
    "x_test = x_test[idx_test,:]\n",
    "y_test = y_test[idx_test]\n",
    "\n",
    "# network parameters\n",
    "input_shape = (original_dim, )\n",
    "intermediate_dim = 512\n",
    "intermediate_dim_2 = 16\n",
    "batch_size = 256\n",
    "latent_dim = 3\n",
    "cat_dim = 1\n",
    "class_num = 3\n",
    "epochs = 50\n",
    "\n",
    "# VAE model = encoder + decoder\n",
    "# build encoder model\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "inter_x1 = Dense(intermediate_dim, activation='relu', name='encoder_intermediate')(inputs)\n",
    "inter_x2 = Dense(intermediate_dim_2, activation='relu', name='encoder_intermediate_2')(inter_x1)\n",
    "inter_x3 = Dense(intermediate_dim_2, activation='relu', name='encoder_intermediate_3')(inter_x1)\n",
    "\n",
    "# add 3 means as additional parameters\n",
    "dummy = Input(shape=(1,), name='dummy')\n",
    "mu_vector = Dense(class_num*latent_dim, name='mu_vector',use_bias=False)(dummy)\n",
    "mu = Reshape((class_num,latent_dim), name='mu')(mu_vector)\n",
    "\n",
    "# prior categorical distribution\n",
    "pi = Dense(class_num, activation='softmax', name='pi')(dummy)\n",
    "\n",
    "# posterior categorical distribution\n",
    "c = Dense(class_num, activation='softmax', name='c')(inter_x2)\n",
    "\n",
    "# outlier/non-outlier classification (Posterior Beta)\n",
    "# inter_outlier = Dense(128, activation='relu', name='inter_outlier')(x)\n",
    "c_outlier = Dense(2, activation='softmax', name='c_outlier')(inter_x3)\n",
    "\n",
    "# q(z|x)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(inter_x2)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(inter_x2)\n",
    "\n",
    "# use reparameterization trick to push the sampling out as input\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "# instantiate encoder model\n",
    "encoder = Model([inputs,dummy], [z_mean, z_log_var, z, mu, c, c_outlier, pi], name='encoder')\n",
    "encoder.summary()\n",
    "\n",
    "# build decoder model\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "inter_y1 = Dense(intermediate_dim_2, activation='relu')(latent_inputs)\n",
    "inter_y2 = Dense(intermediate_dim, activation='relu')(inter_y1)\n",
    "outputs = Dense(original_dim, activation='sigmoid')(inter_y2)\n",
    "\n",
    "# instantiate decoder model\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()\n",
    "#plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)\n",
    "\n",
    "# instantiate VAE model\n",
    "outputs = decoder(encoder([inputs,dummy])[2])\n",
    "vae = Model([inputs,dummy], outputs, name='vae_mlp')\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 784)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dummy (InputLayer)              (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Model)                 [(None, 3), (None, 3 418538      encoder_input[0][0]              \n",
      "                                                                 dummy[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Model)                 (None, 784)          410960      encoder[1][2]                    \n",
      "==================================================================================================\n",
      "Total params: 829,498\n",
      "Trainable params: 829,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/50\n",
      "19707/19707 [==============================] - 4s 182us/step - loss: 81.1999\n",
      "Epoch 2/50\n",
      "19707/19707 [==============================] - 2s 125us/step - loss: 48.0502\n",
      "Epoch 3/50\n",
      "19707/19707 [==============================] - 3s 147us/step - loss: 43.9749\n",
      "Epoch 4/50\n",
      "19707/19707 [==============================] - 4s 179us/step - loss: 41.1634\n",
      "Epoch 5/50\n",
      "19707/19707 [==============================] - 4s 191us/step - loss: 39.6231\n",
      "Epoch 6/50\n",
      "19707/19707 [==============================] - 4s 188us/step - loss: 38.3643\n",
      "Epoch 7/50\n",
      "19707/19707 [==============================] - 4s 211us/step - loss: 37.2863\n",
      "Epoch 8/50\n",
      "19707/19707 [==============================] - 4s 188us/step - loss: 36.3662\n",
      "Epoch 9/50\n",
      "19707/19707 [==============================] - 2s 127us/step - loss: 35.6701\n",
      "Epoch 10/50\n",
      "19707/19707 [==============================] - 3s 139us/step - loss: 35.1121\n",
      "Epoch 11/50\n",
      "19707/19707 [==============================] - 3s 130us/step - loss: 34.5950\n",
      "Epoch 12/50\n",
      "19707/19707 [==============================] - 2s 123us/step - loss: 34.1966\n",
      "Epoch 13/50\n",
      "19707/19707 [==============================] - 2s 127us/step - loss: 33.8296\n",
      "Epoch 14/50\n",
      "19707/19707 [==============================] - 3s 133us/step - loss: 33.3958\n",
      "Epoch 15/50\n",
      "19707/19707 [==============================] - 2s 123us/step - loss: 33.1236\n",
      "Epoch 16/50\n",
      "19707/19707 [==============================] - 2s 126us/step - loss: 32.8127\n",
      "Epoch 17/50\n",
      "19707/19707 [==============================] - 2s 126us/step - loss: 32.5800\n",
      "Epoch 18/50\n",
      "19707/19707 [==============================] - 3s 152us/step - loss: 32.2667\n",
      "Epoch 19/50\n",
      "19707/19707 [==============================] - 3s 161us/step - loss: 32.1013\n",
      "Epoch 20/50\n",
      "19707/19707 [==============================] - 3s 133us/step - loss: 31.8623\n",
      "Epoch 21/50\n",
      "19707/19707 [==============================] - 3s 166us/step - loss: 31.6597\n",
      "Epoch 22/50\n",
      "19707/19707 [==============================] - 3s 137us/step - loss: 31.6448\n",
      "Epoch 23/50\n",
      "19707/19707 [==============================] - 3s 131us/step - loss: 31.4139\n",
      "Epoch 24/50\n",
      "19707/19707 [==============================] - 4s 192us/step - loss: 31.2865\n",
      "Epoch 25/50\n",
      "19707/19707 [==============================] - 3s 159us/step - loss: 31.3600\n",
      "Epoch 26/50\n",
      "19707/19707 [==============================] - 3s 154us/step - loss: 31.1063\n",
      "Epoch 27/50\n",
      "19707/19707 [==============================] - 3s 133us/step - loss: 31.0311\n",
      "Epoch 28/50\n",
      "19707/19707 [==============================] - 3s 128us/step - loss: 30.8321\n",
      "Epoch 29/50\n",
      "19707/19707 [==============================] - 2s 124us/step - loss: 30.7761\n",
      "Epoch 30/50\n",
      "19707/19707 [==============================] - 2s 125us/step - loss: 30.8047\n",
      "Epoch 31/50\n",
      "19707/19707 [==============================] - 2s 124us/step - loss: 30.6810\n",
      "Epoch 32/50\n",
      "19707/19707 [==============================] - 3s 134us/step - loss: 30.5583\n",
      "Epoch 33/50\n",
      "19707/19707 [==============================] - 3s 144us/step - loss: 30.5119\n",
      "Epoch 34/50\n",
      "19707/19707 [==============================] - 3s 142us/step - loss: 30.5445\n",
      "Epoch 35/50\n",
      "19707/19707 [==============================] - 3s 149us/step - loss: 30.3861\n",
      "Epoch 36/50\n",
      "19707/19707 [==============================] - 3s 151us/step - loss: 30.2845\n",
      "Epoch 37/50\n",
      "19707/19707 [==============================] - 4s 194us/step - loss: 30.2203\n",
      "Epoch 38/50\n",
      "19707/19707 [==============================] - 4s 200us/step - loss: 30.2590\n",
      "Epoch 39/50\n",
      "11008/19707 [===============>..............] - ETA: 1s - loss: 30.0297"
     ]
    }
   ],
   "source": [
    "models = (encoder, decoder)\n",
    "dummy_train = np.ones((x_train.shape[0],1))\n",
    "\n",
    "# weight for outlier Beta prior\n",
    "# portion of outlier class\n",
    "Lambda1 = 1\n",
    "Lambda2 = 200 \n",
    "Alpha = 0.025\n",
    "\n",
    "# reconstruction loss\n",
    "reconstruction_loss = mse(inputs, outputs)\n",
    "reconstruction_loss = K.tf.multiply(reconstruction_loss, c_outlier[:,0])\n",
    "reconstruction_loss *= original_dim\n",
    "\n",
    "# sum over reconstruction loss and kl-div loss\n",
    "kl_loss_all = K.tf.get_variable(\"kl_loss_all\", [batch_size,1], \n",
    "                                dtype=K.tf.float32,initializer=K.tf.zeros_initializer)\n",
    "kl_cat_all = K.tf.get_variable(\"kl_cat_all\", [batch_size,1], \n",
    "                                dtype=K.tf.float32,initializer=K.tf.zeros_initializer)\n",
    "dir_prior_all = K.tf.get_variable(\"dir_prior_all\", [batch_size,1], \n",
    "                                  dtype=K.tf.float32,initializer=K.tf.zeros_initializer)\n",
    "\n",
    "for i in range(0,class_num):\n",
    "    # stick-breaking reconstruction of categorical distribution\n",
    "    c_inlier = K.tf.multiply(c[:,i],c_outlier[:,0])\n",
    "    \n",
    "    # kl-divergence between q(z|x) and p(z|c)\n",
    "    kl_loss = 1 + z_log_var - K.square(z_mean-mu[:,i,:]) - K.exp(z_log_var)\n",
    "    kl_loss = K.tf.multiply(K.sum(kl_loss, axis=-1), c_inlier)\n",
    "    kl_loss *= -0.5\n",
    "    kl_loss_all += kl_loss\n",
    "    \n",
    "    # kl-divergence between q(c|x) and p(c) (not including outlier class)\n",
    "    mc = K.mean(c[:,i])\n",
    "    mpi = K.mean(pi[:,i])\n",
    "    kl_cat = mc * K.log(mc)- mc * K.log(mpi)\n",
    "    kl_cat_all += kl_cat\n",
    "    \n",
    "    # Dir prior: Dir(3, 3, ..., 3)\n",
    "    dir_prior = -0.1*K.log(pi[:,i])\n",
    "    dir_prior_all += dir_prior\n",
    "    \n",
    "# kl-divergence between Beta prior and Beta posterior (outlier class)\n",
    "mco1 = K.mean(c_outlier[:,0])\n",
    "mco2 = K.mean(c_outlier[:,1])\n",
    "mpo1 = 1-Alpha\n",
    "mpo2 = Alpha\n",
    "kl_cat_outlier = (mco1 * K.log(mco1)- mco1 * np.log(mpo1)+\n",
    "                  mco2 * K.log(mco2)- mco2 * np.log(mpo2))\n",
    "          \n",
    "# total loss\n",
    "vae_loss = K.mean(reconstruction_loss+\n",
    "                  kl_loss_all+\n",
    "                  dir_prior_all+\n",
    "                  Lambda1*kl_cat_all)+Lambda2*kl_cat_outlier\n",
    "\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    "vae.summary()\n",
    "\n",
    "#vae.load_weights('vae_mlp_mnist.h5')\n",
    "vae.fit([x_train,dummy_train],\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size)\n",
    "vae.save_weights('vae_mlp_mnist.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[z_mean, z_log_var, z, mu, c, c_outlier, pi] = encoder.predict([x_train,dummy_train],batch_size=batch_size)\n",
    "\n",
    "# estimate label\n",
    "\n",
    "labels = np.zeros(x_train.shape[0])\n",
    "for i in range(0, x_train.shape[0]):\n",
    "    max_prob = np.max(np.multiply(c[i,:],c_outlier[i,0]))\n",
    "    idx = np.argmax(np.multiply(c[i,:],c_outlier[i,0]))\n",
    "    if (max_prob > c_outlier[i,1]):\n",
    "        labels[i] = idx\n",
    "    else:\n",
    "        labels[i] = class_num\n",
    "        \n",
    "# plot latent space   \n",
    "#%matplotlib notebook\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(z_mean[:, 0], z_mean[:, 1],z_mean[:,2], c=labels)\n",
    "ax.axis('equal')\n",
    "plt.title('latent space representation')\n",
    "np.sum(labels == 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
