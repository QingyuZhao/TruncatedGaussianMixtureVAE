{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.layers import Lambda, Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "\n",
    "# reparameterization trick\n",
    "# instead of sampling from Q(z|X), sample eps = N(0,I)\n",
    "# z = z_mean + sqrt(var)*eps\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "    # Arguments:\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    # Returns:\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    thre = K.random_uniform(shape=(batch,1))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "def plot_results(models,\n",
    "                 data,\n",
    "                 batch_size=128,\n",
    "                 model_name=\"vae_mnist\"):\n",
    "    \"\"\"Plots labels and MNIST digits as function of 2-dim latent vector\n",
    "    # Arguments:\n",
    "        models (tuple): encoder and decoder models\n",
    "        data (tuple): test data and label\n",
    "        batch_size (int): prediction batch size\n",
    "        model_name (string): which model is using this function\n",
    "    \"\"\"\n",
    "\n",
    "    encoder, decoder = models\n",
    "    x_test, y_test = data\n",
    "    os.makedirs(model_name, exist_ok=True)\n",
    "\n",
    "    filename = os.path.join(model_name, \"vae_mean.png\")\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    z_mean, _, _ = encoder.predict(x_test,\n",
    "                                   batch_size=batch_size)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "    filename = os.path.join(model_name, \"digits_over_latent.png\")\n",
    "    # display a 30x30 2D manifold of digits\n",
    "    n = 30\n",
    "    digit_size = 28\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    # linearly spaced coordinates corresponding to the 2D plot\n",
    "    # of digit classes in the latent space\n",
    "    grid_x = np.linspace(-4, 4, n)\n",
    "    grid_y = np.linspace(-4, 4, n)[::-1]\n",
    "\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample = np.array([[xi, yi]])\n",
    "            x_decoded = decoder.predict(z_sample)\n",
    "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "            figure[i * digit_size: (i + 1) * digit_size,\n",
    "                   j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    start_range = digit_size // 2\n",
    "    end_range = n * digit_size + start_range + 1\n",
    "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "    sample_range_x = np.round(grid_x, 1)\n",
    "    sample_range_y = np.round(grid_y, 1)\n",
    "    plt.xticks(pixel_range, sample_range_x)\n",
    "    plt.yticks(pixel_range, sample_range_y)\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.imshow(figure, cmap='Greys_r')\n",
    "    plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 784)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_intermediate (Dense)    (None, 512)          401920      encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 3)            1539        encoder_intermediate[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 3)            1539        encoder_intermediate[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dummy (InputLayer)              (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (None, 3)            0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mu1 (Dense)                     (None, 3)            3           dummy[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "mu2 (Dense)                     (None, 3)            3           dummy[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "mu3 (Dense)                     (None, 3)            3           dummy[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "c (Dense)                       (None, 3)            1539        encoder_intermediate[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "c_outlier (Dense)               (None, 2)            1026        encoder_intermediate[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pi (Dense)                      (None, 3)            6           dummy[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 407,578\n",
      "Trainable params: 407,578\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z_sampling (InputLayer)      (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 784)               402192    \n",
      "=================================================================\n",
      "Total params: 404,240\n",
      "Trainable params: 404,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 784)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dummy (InputLayer)              (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Model)                 [(None, 3), (None, 3 407578      encoder_input[0][0]              \n",
      "                                                                 dummy[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Model)                 (None, 784)          404240      encoder[1][2]                    \n",
      "==================================================================================================\n",
      "Total params: 811,818\n",
      "Trainable params: 811,818\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "image_size = x_train.shape[1]\n",
    "original_dim = image_size * image_size\n",
    "x_train = np.reshape(x_train, [-1, original_dim])\n",
    "x_test = np.reshape(x_test, [-1, original_dim])\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "idx_train = (y_train == 0) |(y_train == 5) | (y_train == 7) | (np.random.rand(y_train.shape[0]) < 0.05)\n",
    "x_train = x_train[idx_train,:]\n",
    "y_train = y_train[idx_train]\n",
    "idx_test = (y_test == 0) |(y_test == 5) | (y_test == 7) | (np.random.rand(y_test.shape[0]) < 0.05)\n",
    "x_test = x_test[idx_test,:]\n",
    "y_test = y_test[idx_test]\n",
    "\n",
    "# network parameters\n",
    "input_shape = (original_dim, )\n",
    "intermediate_dim = 512\n",
    "batch_size = 256\n",
    "latent_dim = 3\n",
    "cat_dim = 1\n",
    "epochs = 150\n",
    "\n",
    "# VAE model = encoder + decoder\n",
    "# build encoder model\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "x = Dense(intermediate_dim, activation='relu', name='encoder_intermediate')(inputs)\n",
    "\n",
    "# add 3 means as additional parameters\n",
    "dummy = Input(shape=(1,), name='dummy')\n",
    "mu1 = Dense(latent_dim, name='mu1',use_bias=False)(dummy)\n",
    "mu2 = Dense(latent_dim, name='mu2',use_bias=False)(dummy)\n",
    "mu3 = Dense(latent_dim, name='mu3',use_bias=False)(dummy)\n",
    "\n",
    "# prior categorical distribution\n",
    "pi = Dense(3, activation='softmax', name='pi')(dummy)\n",
    "\n",
    "# posterior categorical distribution\n",
    "c = Dense(3, activation='softmax', name='c')(x)\n",
    "\n",
    "# outlier/non-outlier classification (Posterior Beta)\n",
    "#inter_outlier = Dense(128, activation='relu', name='inter_outlier')(x)\n",
    "c_outlier = Dense(2, activation='softmax', name='c_outlier')(x)\n",
    "\n",
    "# q(z|x)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "# use reparameterization trick to push the sampling out as input\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "# instantiate encoder model\n",
    "encoder = Model([inputs,dummy], [z_mean, z_log_var, z, mu1, mu2, mu3, c, c_outlier, pi], name='encoder')\n",
    "encoder.summary()\n",
    "\n",
    "# build decoder model\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "outputs = Dense(original_dim, activation='sigmoid')(x)\n",
    "\n",
    "# instantiate decoder model\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()\n",
    "#plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)\n",
    "\n",
    "# instantiate VAE model\n",
    "outputs = decoder(encoder([inputs,dummy])[2])\n",
    "vae = Model([inputs,dummy], outputs, name='vae_mlp')\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 784)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dummy (InputLayer)              (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Model)                 [(None, 3), (None, 3 407578      encoder_input[0][0]              \n",
      "                                                                 dummy[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Model)                 (None, 784)          404240      encoder[1][2]                    \n",
      "==================================================================================================\n",
      "Total params: 811,818\n",
      "Trainable params: 811,818\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 19813 samples, validate on 3238 samples\n",
      "Epoch 1/150\n",
      "19813/19813 [==============================] - 4s 212us/step - loss: 76.9265 - val_loss: 52.9453\n",
      "Epoch 2/150\n",
      "19813/19813 [==============================] - 3s 133us/step - loss: 47.5319 - val_loss: 45.2191\n",
      "Epoch 3/150\n",
      "19813/19813 [==============================] - 3s 134us/step - loss: 42.2318 - val_loss: 41.9648\n",
      "Epoch 4/150\n",
      "19813/19813 [==============================] - 3s 138us/step - loss: 40.0132 - val_loss: 40.1618\n",
      "Epoch 5/150\n",
      "19813/19813 [==============================] - 3s 137us/step - loss: 38.7396 - val_loss: 39.1750\n",
      "Epoch 6/150\n",
      "19813/19813 [==============================] - 3s 137us/step - loss: 38.1986 - val_loss: 38.5237\n",
      "Epoch 7/150\n",
      "19813/19813 [==============================] - 3s 173us/step - loss: 37.7207 - val_loss: 38.2132\n",
      "Epoch 8/150\n",
      "19813/19813 [==============================] - 3s 165us/step - loss: 37.3018 - val_loss: 37.7743\n",
      "Epoch 9/150\n",
      "19813/19813 [==============================] - 3s 167us/step - loss: 37.0944 - val_loss: 37.5076\n",
      "Epoch 10/150\n",
      "19813/19813 [==============================] - 3s 164us/step - loss: 36.8595 - val_loss: 37.2983\n",
      "Epoch 11/150\n",
      "19813/19813 [==============================] - 3s 175us/step - loss: 36.5664 - val_loss: 37.0902\n",
      "Epoch 12/150\n",
      "19813/19813 [==============================] - 3s 159us/step - loss: 36.2875 - val_loss: 36.6398\n",
      "Epoch 13/150\n",
      "19813/19813 [==============================] - 3s 152us/step - loss: 35.9971 - val_loss: 36.4944\n",
      "Epoch 14/150\n",
      "19813/19813 [==============================] - 3s 172us/step - loss: 35.8445 - val_loss: 35.9776\n",
      "Epoch 15/150\n",
      "19813/19813 [==============================] - 3s 139us/step - loss: 35.5915 - val_loss: 35.9288\n",
      "Epoch 16/150\n",
      "19813/19813 [==============================] - 3s 148us/step - loss: 35.3782 - val_loss: 35.5429\n",
      "Epoch 17/150\n",
      "19813/19813 [==============================] - 3s 149us/step - loss: 35.1666 - val_loss: 35.5327\n",
      "Epoch 18/150\n",
      "19813/19813 [==============================] - 3s 155us/step - loss: 35.0807 - val_loss: 35.6639\n",
      "Epoch 19/150\n",
      "19813/19813 [==============================] - 3s 146us/step - loss: 34.8286 - val_loss: 35.0903\n",
      "Epoch 20/150\n",
      "19813/19813 [==============================] - 3s 133us/step - loss: 34.8603 - val_loss: 35.0361\n",
      "Epoch 21/150\n",
      "19813/19813 [==============================] - 3s 149us/step - loss: 34.6584 - val_loss: 34.8866\n",
      "Epoch 22/150\n",
      "19813/19813 [==============================] - 3s 161us/step - loss: 34.4497 - val_loss: 34.6444\n",
      "Epoch 23/150\n",
      "19813/19813 [==============================] - 3s 133us/step - loss: 34.3660 - val_loss: 34.5463\n",
      "Epoch 24/150\n",
      "19813/19813 [==============================] - 3s 160us/step - loss: 34.2276 - val_loss: 34.3973\n",
      "Epoch 25/150\n",
      "19813/19813 [==============================] - 3s 147us/step - loss: 34.1572 - val_loss: 34.6086\n",
      "Epoch 26/150\n",
      "19813/19813 [==============================] - 3s 144us/step - loss: 34.0427 - val_loss: 34.7010\n",
      "Epoch 27/150\n",
      "19813/19813 [==============================] - 3s 136us/step - loss: 34.0643 - val_loss: 34.2880\n",
      "Epoch 28/150\n",
      "19813/19813 [==============================] - 3s 137us/step - loss: 33.9482 - val_loss: 34.2969\n",
      "Epoch 29/150\n",
      "19813/19813 [==============================] - 3s 136us/step - loss: 33.8204 - val_loss: 34.0905\n",
      "Epoch 30/150\n",
      "19813/19813 [==============================] - 3s 138us/step - loss: 33.9635 - val_loss: 34.1034\n",
      "Epoch 31/150\n",
      "19813/19813 [==============================] - 3s 138us/step - loss: 33.6471 - val_loss: 33.8730\n",
      "Epoch 32/150\n",
      "19813/19813 [==============================] - 3s 138us/step - loss: 33.6166 - val_loss: 34.0077\n",
      "Epoch 33/150\n",
      "19813/19813 [==============================] - 3s 143us/step - loss: 33.6400 - val_loss: 33.8508\n",
      "Epoch 34/150\n",
      "19813/19813 [==============================] - 3s 138us/step - loss: 33.5530 - val_loss: 33.7115\n",
      "Epoch 35/150\n",
      "19813/19813 [==============================] - 3s 141us/step - loss: 33.4359 - val_loss: 33.7160\n",
      "Epoch 36/150\n",
      "19813/19813 [==============================] - 3s 137us/step - loss: 33.4843 - val_loss: 33.6633\n",
      "Epoch 37/150\n",
      "19813/19813 [==============================] - 3s 141us/step - loss: 33.3699 - val_loss: 33.5246\n",
      "Epoch 38/150\n",
      "19813/19813 [==============================] - 3s 139us/step - loss: 33.3465 - val_loss: 33.7321\n",
      "Epoch 39/150\n",
      "19813/19813 [==============================] - 3s 141us/step - loss: 33.3133 - val_loss: 33.5153\n",
      "Epoch 40/150\n",
      "19813/19813 [==============================] - 3s 140us/step - loss: 33.2611 - val_loss: 33.5669\n",
      "Epoch 41/150\n",
      "19813/19813 [==============================] - 3s 141us/step - loss: 33.1827 - val_loss: 33.3594\n",
      "Epoch 42/150\n",
      "19813/19813 [==============================] - 3s 141us/step - loss: 33.0610 - val_loss: 33.3875\n",
      "Epoch 43/150\n",
      "19813/19813 [==============================] - 3s 141us/step - loss: 33.1118 - val_loss: 33.3169\n",
      "Epoch 44/150\n",
      "19813/19813 [==============================] - 3s 141us/step - loss: 33.2446 - val_loss: 33.4299\n",
      "Epoch 45/150\n",
      "19813/19813 [==============================] - 3s 137us/step - loss: 33.1251 - val_loss: 33.2997\n",
      "Epoch 46/150\n",
      "19813/19813 [==============================] - 3s 134us/step - loss: 33.0387 - val_loss: 33.1909\n",
      "Epoch 47/150\n",
      "19813/19813 [==============================] - 3s 135us/step - loss: 33.0056 - val_loss: 33.1895\n",
      "Epoch 48/150\n",
      "19813/19813 [==============================] - 3s 146us/step - loss: 32.9094 - val_loss: 33.1676\n",
      "Epoch 49/150\n",
      "19813/19813 [==============================] - 3s 156us/step - loss: 32.9416 - val_loss: 33.3133\n",
      "Epoch 50/150\n",
      "19813/19813 [==============================] - 3s 135us/step - loss: 32.9506 - val_loss: 33.3037\n",
      "Epoch 51/150\n",
      "19813/19813 [==============================] - 3s 137us/step - loss: 32.9617 - val_loss: 33.4249\n",
      "Epoch 52/150\n",
      "19813/19813 [==============================] - 3s 137us/step - loss: 32.8825 - val_loss: 33.1230\n",
      "Epoch 53/150\n",
      "19813/19813 [==============================] - 3s 135us/step - loss: 32.7856 - val_loss: 33.0361\n",
      "Epoch 54/150\n",
      "19813/19813 [==============================] - 3s 138us/step - loss: 32.7853 - val_loss: 33.1229\n",
      "Epoch 55/150\n",
      "19813/19813 [==============================] - 3s 137us/step - loss: 32.7488 - val_loss: 32.9648\n",
      "Epoch 56/150\n",
      "19813/19813 [==============================] - 3s 135us/step - loss: 32.8228 - val_loss: 33.1143\n",
      "Epoch 57/150\n",
      "19813/19813 [==============================] - 3s 136us/step - loss: 32.7244 - val_loss: 33.0110\n",
      "Epoch 58/150\n",
      "19813/19813 [==============================] - 3s 140us/step - loss: 32.6938 - val_loss: 32.9828\n",
      "Epoch 59/150\n",
      "19813/19813 [==============================] - 3s 157us/step - loss: 32.7168 - val_loss: 33.2100\n",
      "Epoch 60/150\n",
      "19813/19813 [==============================] - 3s 137us/step - loss: 32.7292 - val_loss: 32.9942\n",
      "Epoch 61/150\n",
      "19813/19813 [==============================] - 3s 137us/step - loss: 32.6997 - val_loss: 33.0518\n",
      "Epoch 62/150\n",
      "19813/19813 [==============================] - 3s 137us/step - loss: 32.7748 - val_loss: 33.1167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/150\n",
      "19813/19813 [==============================] - 3s 128us/step - loss: 32.5900 - val_loss: 32.7719\n",
      "Epoch 64/150\n",
      "19813/19813 [==============================] - 3s 128us/step - loss: 32.5636 - val_loss: 32.7424\n",
      "Epoch 65/150\n",
      "19813/19813 [==============================] - 3s 130us/step - loss: 32.5385 - val_loss: 32.7452\n",
      "Epoch 66/150\n",
      "19813/19813 [==============================] - 3s 131us/step - loss: 32.5065 - val_loss: 32.8353\n",
      "Epoch 67/150\n",
      "19813/19813 [==============================] - 3s 131us/step - loss: 32.5515 - val_loss: 32.7785\n",
      "Epoch 68/150\n",
      "19813/19813 [==============================] - 3s 129us/step - loss: 32.5119 - val_loss: 32.9711\n",
      "Epoch 69/150\n",
      "19813/19813 [==============================] - 3s 129us/step - loss: 32.5947 - val_loss: 32.7207\n",
      "Epoch 70/150\n",
      "19813/19813 [==============================] - 3s 129us/step - loss: 32.5101 - val_loss: 32.9995\n",
      "Epoch 71/150\n",
      "19813/19813 [==============================] - 3s 129us/step - loss: 32.4568 - val_loss: 32.7345\n",
      "Epoch 72/150\n",
      "19813/19813 [==============================] - 3s 130us/step - loss: 32.5134 - val_loss: 32.7168\n",
      "Epoch 73/150\n",
      "19813/19813 [==============================] - 3s 130us/step - loss: 32.4326 - val_loss: 32.7329\n",
      "Epoch 74/150\n",
      "19813/19813 [==============================] - 3s 129us/step - loss: 32.3990 - val_loss: 32.7500\n",
      "Epoch 75/150\n",
      "19813/19813 [==============================] - 3s 129us/step - loss: 32.3759 - val_loss: 32.7772\n",
      "Epoch 76/150\n",
      "19813/19813 [==============================] - 3s 129us/step - loss: 32.3697 - val_loss: 33.0970\n",
      "Epoch 77/150\n",
      "19813/19813 [==============================] - 3s 130us/step - loss: 32.3139 - val_loss: 32.5471\n",
      "Epoch 78/150\n",
      "19813/19813 [==============================] - 3s 131us/step - loss: 32.3199 - val_loss: 32.6231\n",
      "Epoch 79/150\n",
      "19813/19813 [==============================] - 3s 132us/step - loss: 32.3504 - val_loss: 32.9021\n",
      "Epoch 80/150\n",
      "19813/19813 [==============================] - 3s 132us/step - loss: 32.3915 - val_loss: 32.5067\n",
      "Epoch 81/150\n",
      "19813/19813 [==============================] - 3s 130us/step - loss: 32.3210 - val_loss: 32.5304\n",
      "Epoch 82/150\n",
      "19813/19813 [==============================] - 3s 132us/step - loss: 32.2484 - val_loss: 32.6598\n",
      "Epoch 83/150\n",
      "19813/19813 [==============================] - 3s 132us/step - loss: 32.1953 - val_loss: 32.6051\n",
      "Epoch 84/150\n",
      "19813/19813 [==============================] - 3s 132us/step - loss: 32.2693 - val_loss: 32.6341\n",
      "Epoch 85/150\n",
      "19813/19813 [==============================] - 3s 132us/step - loss: 32.2445 - val_loss: 32.4641\n",
      "Epoch 86/150\n",
      "19813/19813 [==============================] - 3s 133us/step - loss: 32.2221 - val_loss: 32.6112\n",
      "Epoch 87/150\n",
      "19813/19813 [==============================] - 3s 131us/step - loss: 32.1642 - val_loss: 32.5019\n",
      "Epoch 88/150\n",
      "19813/19813 [==============================] - 3s 131us/step - loss: 32.1687 - val_loss: 32.4546\n",
      "Epoch 89/150\n",
      "19813/19813 [==============================] - 3s 132us/step - loss: 32.2347 - val_loss: 32.6126\n",
      "Epoch 90/150\n",
      "19813/19813 [==============================] - 3s 131us/step - loss: 32.2004 - val_loss: 32.4623\n",
      "Epoch 91/150\n",
      "19813/19813 [==============================] - 3s 131us/step - loss: 32.1025 - val_loss: 32.7866\n",
      "Epoch 92/150\n",
      "19813/19813 [==============================] - 3s 131us/step - loss: 32.1466 - val_loss: 32.6326\n",
      "Epoch 93/150\n",
      "19813/19813 [==============================] - 3s 136us/step - loss: 32.1344 - val_loss: 32.5166\n",
      "Epoch 94/150\n",
      "19813/19813 [==============================] - 3s 133us/step - loss: 32.1098 - val_loss: 32.5357\n",
      "Epoch 95/150\n",
      "19813/19813 [==============================] - 3s 134us/step - loss: 32.1477 - val_loss: 32.4908\n",
      "Epoch 96/150\n",
      "19813/19813 [==============================] - 3s 132us/step - loss: 32.1127 - val_loss: 32.3812\n",
      "Epoch 97/150\n",
      "19813/19813 [==============================] - 3s 133us/step - loss: 32.1425 - val_loss: 32.5836\n",
      "Epoch 98/150\n",
      "19813/19813 [==============================] - 3s 141us/step - loss: 32.0726 - val_loss: 32.4656\n",
      "Epoch 99/150\n",
      "19813/19813 [==============================] - 3s 156us/step - loss: 32.1216 - val_loss: 32.3599\n",
      "Epoch 100/150\n",
      "19813/19813 [==============================] - 3s 146us/step - loss: 32.0519 - val_loss: 32.4617\n",
      "Epoch 101/150\n",
      "19813/19813 [==============================] - 3s 143us/step - loss: 32.0153 - val_loss: 32.8398\n",
      "Epoch 102/150\n",
      "19813/19813 [==============================] - 2s 123us/step - loss: 31.9614 - val_loss: 32.3178\n",
      "Epoch 103/150\n",
      "19813/19813 [==============================] - 3s 132us/step - loss: 32.0119 - val_loss: 32.3357\n",
      "Epoch 104/150\n",
      "19813/19813 [==============================] - 3s 133us/step - loss: 31.9414 - val_loss: 32.3669\n",
      "Epoch 105/150\n",
      "19813/19813 [==============================] - 2s 126us/step - loss: 31.9544 - val_loss: 32.4612\n",
      "Epoch 106/150\n",
      "19813/19813 [==============================] - 2s 116us/step - loss: 31.8853 - val_loss: 32.3769\n",
      "Epoch 107/150\n",
      "19813/19813 [==============================] - 2s 124us/step - loss: 31.9209 - val_loss: 32.3317\n",
      "Epoch 108/150\n",
      "19813/19813 [==============================] - 2s 118us/step - loss: 31.9101 - val_loss: 32.3074\n",
      "Epoch 109/150\n",
      "19813/19813 [==============================] - 3s 135us/step - loss: 31.9494 - val_loss: 32.5621\n",
      "Epoch 110/150\n",
      "19813/19813 [==============================] - 2s 118us/step - loss: 31.9715 - val_loss: 32.4076\n",
      "Epoch 111/150\n",
      "19813/19813 [==============================] - 3s 151us/step - loss: 31.9282 - val_loss: 32.4025\n",
      "Epoch 112/150\n",
      "19813/19813 [==============================] - 3s 152us/step - loss: 31.9067 - val_loss: 32.2955\n",
      "Epoch 113/150\n",
      "19813/19813 [==============================] - 3s 133us/step - loss: 31.8867 - val_loss: 32.4635\n",
      "Epoch 114/150\n",
      "19813/19813 [==============================] - 3s 133us/step - loss: 31.8918 - val_loss: 32.3182\n",
      "Epoch 115/150\n",
      "19813/19813 [==============================] - 3s 136us/step - loss: 31.9075 - val_loss: 32.4928\n",
      "Epoch 116/150\n",
      "19813/19813 [==============================] - 3s 133us/step - loss: 31.9394 - val_loss: 32.2901\n",
      "Epoch 117/150\n",
      "19813/19813 [==============================] - 2s 120us/step - loss: 31.8813 - val_loss: 32.3535\n",
      "Epoch 118/150\n",
      "19813/19813 [==============================] - 2s 120us/step - loss: 31.8721 - val_loss: 32.3199\n",
      "Epoch 119/150\n",
      "19813/19813 [==============================] - 2s 116us/step - loss: 31.8826 - val_loss: 32.3046\n",
      "Epoch 120/150\n",
      "19813/19813 [==============================] - 2s 116us/step - loss: 31.8692 - val_loss: 32.2166\n",
      "Epoch 121/150\n",
      "19813/19813 [==============================] - 3s 128us/step - loss: 31.7875 - val_loss: 32.4170\n",
      "Epoch 122/150\n",
      "19813/19813 [==============================] - 2s 117us/step - loss: 31.8794 - val_loss: 32.3261\n",
      "Epoch 123/150\n",
      "19813/19813 [==============================] - 3s 129us/step - loss: 31.8714 - val_loss: 32.5093\n",
      "Epoch 124/150\n",
      "19813/19813 [==============================] - 2s 119us/step - loss: 31.8412 - val_loss: 32.3198\n",
      "Epoch 125/150\n",
      "19813/19813 [==============================] - 3s 131us/step - loss: 31.7948 - val_loss: 32.2893\n",
      "Epoch 126/150\n",
      "19813/19813 [==============================] - 3s 175us/step - loss: 31.8206 - val_loss: 32.4631\n",
      "Epoch 127/150\n",
      "19813/19813 [==============================] - 3s 150us/step - loss: 31.8256 - val_loss: 32.2569\n",
      "Epoch 128/150\n",
      "19813/19813 [==============================] - 3s 133us/step - loss: 31.7995 - val_loss: 32.2331\n",
      "Epoch 129/150\n",
      "19813/19813 [==============================] - 3s 127us/step - loss: 31.7811 - val_loss: 32.2769\n",
      "Epoch 130/150\n",
      "19813/19813 [==============================] - 2s 119us/step - loss: 31.9385 - val_loss: 32.3655\n",
      "Epoch 131/150\n",
      "19813/19813 [==============================] - 2s 115us/step - loss: 31.7174 - val_loss: 32.1687\n",
      "Epoch 132/150\n",
      "19813/19813 [==============================] - 2s 116us/step - loss: 31.7652 - val_loss: 32.2169\n",
      "Epoch 133/150\n",
      "19813/19813 [==============================] - 3s 129us/step - loss: 31.7677 - val_loss: 32.3390\n",
      "Epoch 134/150\n",
      "19813/19813 [==============================] - 3s 130us/step - loss: 31.7058 - val_loss: 32.1422\n",
      "Epoch 135/150\n",
      "19813/19813 [==============================] - 3s 146us/step - loss: 31.7524 - val_loss: 32.1566\n",
      "Epoch 136/150\n",
      "19813/19813 [==============================] - 2s 125us/step - loss: 31.7231 - val_loss: 32.1734\n",
      "Epoch 137/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19813/19813 [==============================] - 3s 137us/step - loss: 31.7097 - val_loss: 32.2473\n",
      "Epoch 138/150\n",
      "19813/19813 [==============================] - 2s 124us/step - loss: 31.6709 - val_loss: 32.1880\n",
      "Epoch 139/150\n",
      "19813/19813 [==============================] - 2s 121us/step - loss: 31.7292 - val_loss: 32.2595\n",
      "Epoch 140/150\n",
      "19813/19813 [==============================] - 2s 122us/step - loss: 31.7762 - val_loss: 32.1028\n",
      "Epoch 141/150\n",
      "19813/19813 [==============================] - 3s 137us/step - loss: 31.7535 - val_loss: 32.2527\n",
      "Epoch 142/150\n",
      "19813/19813 [==============================] - 2s 125us/step - loss: 31.6637 - val_loss: 32.1011\n",
      "Epoch 143/150\n",
      "19813/19813 [==============================] - 2s 116us/step - loss: 31.6761 - val_loss: 32.0927\n",
      "Epoch 144/150\n",
      "19813/19813 [==============================] - 3s 126us/step - loss: 31.7096 - val_loss: 32.0770\n",
      "Epoch 145/150\n",
      "19813/19813 [==============================] - 3s 128us/step - loss: 31.6433 - val_loss: 32.0760\n",
      "Epoch 146/150\n",
      "19813/19813 [==============================] - 2s 119us/step - loss: 31.6173 - val_loss: 32.1004\n",
      "Epoch 147/150\n",
      "19813/19813 [==============================] - 3s 134us/step - loss: 31.6305 - val_loss: 32.3851\n",
      "Epoch 148/150\n",
      "19813/19813 [==============================] - 3s 164us/step - loss: 31.6111 - val_loss: 32.1591\n",
      "Epoch 149/150\n",
      "19813/19813 [==============================] - 3s 174us/step - loss: 31.5705 - val_loss: 32.0779\n",
      "Epoch 150/150\n",
      "19813/19813 [==============================] - 4s 196us/step - loss: 31.6375 - val_loss: 32.1433\n"
     ]
    }
   ],
   "source": [
    "models = (encoder, decoder)\n",
    "data = (x_test, y_test)\n",
    "dummy_train = np.ones((y_train.shape[0],1))\n",
    "dummy_test  = np.ones((y_test.shape[0],1))\n",
    "\n",
    "# weight for outlier Beta prior\n",
    "# portion of outlier class\n",
    "Lambda = 100\n",
    "Alpha = 0.075\n",
    "\n",
    "# stick-breaking reconstruction of categorical distribution\n",
    "c0 = K.tf.multiply(c[:,0],c_outlier[:,0])\n",
    "c1 = K.tf.multiply(c[:,1],c_outlier[:,0])\n",
    "c2 = K.tf.multiply(c[:,2],c_outlier[:,0])\n",
    "c3 = c_outlier[:,1]\n",
    "\n",
    "# reconstruction loss\n",
    "reconstruction_loss = mse(inputs, outputs)\n",
    "reconstruction_loss = K.tf.multiply(reconstruction_loss, c0 + c1 + c2)\n",
    "reconstruction_loss *= original_dim\n",
    "\n",
    "# kl-divergence between q(z|x) and p(z|c)\n",
    "kl_loss_1 = 1 + z_log_var - K.square(z_mean-mu1) - K.exp(z_log_var)\n",
    "kl_loss_1 = K.tf.multiply(K.sum(kl_loss_1, axis=-1) , c0)\n",
    "kl_loss_2 = 1 + z_log_var - K.square(z_mean-mu2) - K.exp(z_log_var)\n",
    "kl_loss_2 = K.tf.multiply(K.sum(kl_loss_2, axis=-1) , c1)\n",
    "kl_loss_3 = 1 + z_log_var - K.square(z_mean-mu3) - K.exp(z_log_var)\n",
    "kl_loss_3 = K.tf.multiply(K.sum(kl_loss_3, axis=-1) , c2)\n",
    "kl_loss_1 *= -0.5\n",
    "kl_loss_2 *= -0.5\n",
    "kl_loss_3 *= -0.5\n",
    "\n",
    "# kl-divergence between q(c|x) and p(c) (not including outlier class)\n",
    "mc1 = K.mean(c[:,0])\n",
    "mc2 = K.mean(c[:,1])\n",
    "mc3 = K.mean(c[:,2])\n",
    "mpi1 = K.mean(pi[:,0])\n",
    "mpi2 = K.mean(pi[:,1])\n",
    "mpi3 = K.mean(pi[:,2])\n",
    "kl_cat = (mc1 * K.log(mc1)- mc1 * K.log(mpi1)+\n",
    "          mc2 * K.log(mc2)- mc2 * K.log(mpi2)+\n",
    "          mc3 * K.log(mc3)- mc3 * K.log(mpi3))\n",
    "\n",
    "# kl-divergence between Beta prior and Beta posterior (outlier class)\n",
    "mco1 = K.mean(c_outlier[:,0])\n",
    "mco2 = K.mean(c_outlier[:,1])\n",
    "mpo1 = 1-Alpha\n",
    "mpo2 = Alpha\n",
    "kl_cat_outlier = (mco1 * K.log(mco1)- mco1 * np.log(mpo1)+\n",
    "                  mco2 * K.log(mco2)- mco2 * np.log(mpo2))\n",
    "          \n",
    "# Dir prior  Dir(3, 3, 3)\n",
    "dir_prior =  -2*K.log(pi[:,0])-2*K.log(pi[:,1])-2*K.log(pi[:,2])\n",
    "\n",
    "# total loss\n",
    "vae_loss = K.mean(reconstruction_loss+\n",
    "                  kl_loss_1+kl_loss_2+kl_loss_3+\n",
    "                  dir_prior+kl_cat+Lambda*kl_cat_outlier)\n",
    "\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    "vae.summary()\n",
    "\n",
    "#vae.load_weights('vae_mlp_mnist.h5')\n",
    "vae.fit([x_train,dummy_train],\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=([x_test,dummy_test], None))\n",
    "vae.save_weights('vae_mlp_mnist.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[z_mean, z_log_var, z, mu1, mu2, mu3, c_1, c_2, pi] = encoder.predict([x_test,dummy_test],batch_size=batch_size)\n",
    "\n",
    "# estimate label\n",
    "c0 = c_1[:,0] * c_2[:,0]\n",
    "c1 = c_1[:,1] * c_2[:,0]\n",
    "c2 = c_1[:,2] * c_2[:,0]\n",
    "c3 = c_2[:,1]\n",
    "\n",
    "color = np.ones(y_test.shape[0])*3\n",
    "for i in range(0, y_test.shape[0]):\n",
    "    if (c0[i] > c1[i]) and (c0[i] > c2[i]) and (c0[i] > c3[i]):\n",
    "        color[i] = 0\n",
    "    if (c1[i] > c0[i]) and (c1[i] > c2[i]) and (c1[i] > c3[i]):\n",
    "        color[i] = 1\n",
    "    if (c2[i] > c0[i]) and (c2[i] > c1[i]) and (c2[i] > c3[i]):\n",
    "        color[i] = 2\n",
    "        \n",
    "# plot using computed or ground-truth label\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib notebook\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "color_gt = y_test.copy()\n",
    "color_gt[(color_gt!=0) & (color_gt!=5) & (color_gt!=7)] = 9\n",
    "color_gt[(color_gt==0)]=0;\n",
    "color_gt[(color_gt==5)]=1;\n",
    "color_gt[(color_gt==7)]=2;\n",
    "color_gt[(color_gt==9)]=3;\n",
    "\n",
    "\n",
    "ax.scatter(z_mean[:, 0], z_mean[:, 1],z_mean[:,2], c=color_gt)\n",
    "ax.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
